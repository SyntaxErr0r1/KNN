{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvhuDvTUGJUF"
   },
   "source": [
    "#EasyNMT - Example (Opus-MT Model)\n",
    "This notebook shows the usage of [EasyNMT](https://github.com/UKPLab/EasyNMT) for machine translation.\n",
    "\n",
    "Here, we use the [Opus-MT model](https://github.com/Helsinki-NLP/Opus-MT). The Helsiniki-NLP group provides 1200+ pre-trained models for various language directions (e.g. en-de, es-fr, ru-fr). Each model has a size of about 300 MB.\n",
    "\n",
    "We make the usage of the models easy: The suitable model needed for your translation is loaded automatically and kept in memory for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sc745ZyOIhKF",
    "outputId": "c8a2946b-51d8-42c5-81bc-1b05f3969ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 27 00:47:21 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:2F:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    69W / 400W |      0MiB / 40960MiB |      3%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo5irA2MZSsk"
   },
   "source": [
    "### Clean badboy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "ZgJ2anS3ZSHx",
    "outputId": "7081ff1d-b5b7-4d4d-fb71-1caca419c500"
   },
   "outputs": [],
   "source": [
    "# # Run this to clean GPU memory\n",
    "import torch\n",
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KUU4nqKkXPD"
   },
   "outputs": [],
   "source": [
    "!pip install -U easynmt\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlpteWMEVtJL"
   },
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qh7kAqp3kjS2"
   },
   "outputs": [],
   "source": [
    "from easynmt import EasyNMT\n",
    "model = EasyNMT('opus-mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXaDCZ96N2-J",
    "outputId": "ed18d32b-36f1-4811-efc2-a994b3f3e6c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/matej/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\") # TODO: zahodit do pice lebo public repo xd, HF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e5mJjGSY7ew1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a753a418be34932af0d0ad71d311776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790e56f1c0c249c2a70cf665b9a66a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 163M/163M [00:08<00:00, 19.5MB/s] \n",
      "Downloading data: 100%|██████████| 145M/145M [00:06<00:00, 22.5MB/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a27c441b9747c09c48d9a20cf91f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/363491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Open-Orca/SlimOrca-Dedup\", split='train')\n",
    "\n",
    "# TODO process dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "a9da11d292e7441aaaf7725ae4d2508f",
      "0517a0341e2b4bdaade5bbf0fddb5fca",
      "567b92b20a3d454aa958708733d83f28",
      "d6b15a0c4dee4b6a91ebb40d2247298b",
      "82d187090d964bedbf0ca4002376cecf",
      "50bff47e4a424d2588460e39ab68c6ce",
      "3cfb90d9325140f2bc4519c416087674",
      "a6d28483343743b38ff2788a5bb346e9",
      "d962b725c65d4250abc38021ab3b0ff3",
      "e9ad199c6fb2474ab8fd7915e8f4812f",
      "e857d285567f4a6b9c293aa331077b21"
     ]
    },
    "id": "ypIAEdtsdgxs",
    "outputId": "f4bc0b66-1994-459c-8bd4-3cffda84b65f"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(300_000, 300_100))# dataset.to_json(\"dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m input_text \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m response \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/googletrans/client.py:182\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    181\u001b[0m origin \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m--> 182\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# this code will be updated when the format is changed.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/googletrans/client.py:78\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_translate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, dest, src, override):\n\u001b[0;32m---> 78\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_acquirer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[1;32m     80\u001b[0m                                 token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[1;32m     82\u001b[0m     url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/googletrans/gtoken.py:194\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire(text)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/googletrans/gtoken.py:62\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRE_TKK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode-escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "# get sample\n",
    "sample = dataset[0]\n",
    "sys_val = sample[\"conversations\"][0][\"value\"]\n",
    "input_text = sample[\"conversations\"][1][\"value\"]\n",
    "response = sample[\"conversations\"][2][\"value\"]\n",
    "\n",
    "result = translator.translate(sys_val, dest='cs', src='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161967\n",
      "Jste asistent s umělou inteligencí, který pomáhá lidem vyhledávat informace. Uživatel vám zadá otázku. Vaším úkolem je co nejvěrněji odpovědět. Při odpovídání přemýšlejte krok za krokem a svou odpověď zdůvodněte.\n"
     ]
    }
   ],
   "source": [
    "# count characters for 100 dataset inputs\n",
    "count_chars = 0\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    sys_val = sample[\"conversations\"][0][\"value\"]\n",
    "    input_text = sample[\"conversations\"][1][\"value\"]\n",
    "    response = sample[\"conversations\"][2][\"value\"]\n",
    "    count_chars += len(sys_val) + len(input_text) + len(response)\n",
    "\n",
    "print(count_chars)\n",
    "\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yX1bHhnIUtUd"
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "# import torch\n",
    "# from typing import List\n",
    "\n",
    "\n",
    "# class OpusMT:\n",
    "#     def __init__(self, easynmt_path: str = None, max_loaded_models: int = 10):\n",
    "#         self.models = {}\n",
    "#         self.max_loaded_models = max_loaded_models\n",
    "#         self.max_length = None\n",
    "\n",
    "#     def load_model(self, model_name):\n",
    "#         if model_name in self.models:\n",
    "#             self.models[model_name]['last_loaded'] = time.time()\n",
    "#             return self.models[model_name]['tokenizer'], self.models[model_name]['model']\n",
    "#         else:\n",
    "#             tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "#             model = MarianMTModel.from_pretrained(model_name)\n",
    "#             model.eval()\n",
    "\n",
    "#             if len(self.models) >= self.max_loaded_models:\n",
    "#                 oldest_time = time.time()\n",
    "#                 oldest_model = None\n",
    "#                 for loaded_model_name in self.models:\n",
    "#                     if self.models[loaded_model_name]['last_loaded'] <= oldest_time:\n",
    "#                         oldest_model = loaded_model_name\n",
    "#                         oldest_time = self.models[loaded_model_name]['last_loaded']\n",
    "#                 del self.models[oldest_model]\n",
    "\n",
    "#             self.models[model_name] = {'tokenizer': tokenizer, 'model': model, 'last_loaded': time.time()}\n",
    "#             return tokenizer, model\n",
    "\n",
    "#     def translate_sentences(self, sentences: List[str], source_lang: str, target_lang: str, device: str, beam_size: int = 5, **kwargs):\n",
    "#         model_name = 'Helsinki-NLP/opus-mt-{}-{}'.format(source_lang, target_lang)\n",
    "#         tokenizer, model = self.load_model(model_name)\n",
    "#         model.to(device)\n",
    "\n",
    "#         inputs = tokenizer(sentences, truncation=True, padding=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "#         for key in inputs:\n",
    "#             inputs[key] = inputs[key].to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             translated = model.generate(**inputs, num_beams=beam_size, **kwargs)\n",
    "#             # print((translated.shape))\n",
    "#             output = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "#         return output\n",
    "\n",
    "#     def save(self, output_path):\n",
    "#         return {\"max_loaded_models\": self.max_loaded_models}\n",
    "\n",
    "# opes_cz = OpusMT()\n",
    "# tokenizer_test, _ = opes_cz.load_model('Helsinki-NLP/opus-mt-en-cs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB4i7NgGJvFm"
   },
   "source": [
    "# Document Translation\n",
    "You can also pass longer documents (or list of documents) to the `translate()` method.\n",
    "\n",
    "As Transformer models can only translate inputs up to 512 (or 1024) word pieces, we first perform sentence splitting. Then, each sentence is translated individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKo8PaxtKHzP",
    "outputId": "3ddb300a-d7ea-408e-fcc9-cddeb705b39f"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "document_1 = \"\"\"\n",
    "Berlin is the capital and largest city of Germany by both area and population.\n",
    "Its 3,769,495 inhabitants as of 31 December 2019 make it the most-populous city of the European Union, according to population within city limits.\n",
    "The city is also one of Germany's 16 federal states. It is surrounded by the state of Brandenburg, and contiguous with Potsdam, Brandenburg's capital.\n",
    "The two cities are at the center of the Berlin-Brandenburg capital region, which is, with about six million inhabitants and an area of more than 30,000 km2, Germany's third-largest metropolitan region after the Rhine-Ruhr and Rhine-Main regions.\n",
    "Berlin straddles the banks of the River Spree, which flows into the River Havel (a tributary of the River Elbe) in the western borough of Spandau.\n",
    "Among the city's main topographical features are the many lakes in the western and southeastern boroughs formed by the Spree, Havel, and Dahme rivers (the largest of which is Lake Müggelsee).\n",
    "\"\"\"\n",
    "# Due to its location in the European Plain, Berlin is influenced by a temperate seasonal climate.\n",
    "# About one-third of the city's area is composed of forests, parks, gardens, rivers, canals and lakes.\n",
    "# The city lies in the Central German dialect area, the Berlin dialect being a variant of the Lusatian-New Marchian dialects.\n",
    "\n",
    "# First documented in the 13th century and at the crossing of two important historic trade routes, Berlin became the capital of the Margraviate of Brandenburg (1417–1701), the Kingdom of Prussia (1701–1918), the German Empire (1871–1918), the Weimar Republic (1919–1933), and the Third Reich (1933–1945).\n",
    "# Berlin in the 1920s was the third-largest municipality in the world.\n",
    "# After World War II and its subsequent occupation by the victorious countries, the city was divided; West Berlin became a de facto West German exclave, surrounded by the Berlin Wall (1961–1989) and East German territory.\n",
    "# East Berlin was declared capital of East Germany, while Bonn became the West German capital.\n",
    "# Following German reunification in 1990, Berlin once again became the capital of all of Germany.\n",
    "\n",
    "# Berlin is a world city of culture, politics, media and science.\n",
    "# Its economy is based on high-tech firms and the service sector, encompassing a diverse range of creative industries, research facilities, media corporations and convention venues.\n",
    "# Berlin serves as a continental hub for air and rail traffic and has a highly complex public transportation network.\n",
    "# The metropolis is a popular tourist destination.\n",
    "# Significant industries also include IT, pharmaceuticals, biomedical engineering, clean tech, biotechnology, construction and electronics.\n",
    "# \"\"\"\n",
    "\n",
    "document_2 = \"\"\"\n",
    "Please add spaces between words: ThereweretheheelsofforeigninvasiontrampinguponFrance;therewasthedownfallofauEmpire,andthecaptivityofaBonaparte;andtheretheywerethemselves.\n",
    "\"\"\"\n",
    "print(tokenizer_test(document_2, truncation=True, padding=True, max_length=None, return_tensors=\"pt\")[\"input_ids\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_l5lEyVap6B"
   },
   "outputs": [],
   "source": [
    "# nebere to poslednu vetu ? HALO\n",
    "print(opes_cz.translate_sentences([document_2], source_lang=\"en\", target_lang=\"cs\", beam_size=20, device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lfrjkf_dWHRy"
   },
   "outputs": [],
   "source": [
    "print(\"Output:\")\n",
    "print(model.translate(document_2, target_lang='cs', perform_sentence_splitting=True, beam_size=15, batch_size=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SLdm2TQbSTk"
   },
   "source": [
    "## Translate dataset\n",
    "- save to `translated_dataset.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "m-55oKixQZen",
    "outputId": "cf557918-0c69-4e54-b60a-9d9aecf924ed"
   },
   "outputs": [],
   "source": [
    "# translate all samples from dataset\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "output_path = \"translated_dataset.json\"\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "# Open the output file for writing\n",
    "print(f\"Translating dataset of size: {len(dataset)}\")\n",
    "with open(output_path, \"a\", encoding=\"utf-8\") as output_file:\n",
    "    start_time = time.time()\n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]\n",
    "        sys_val = sample[\"conversations\"][0][\"value\"]\n",
    "        input_text = sample[\"conversations\"][1][\"value\"]\n",
    "        response = sample[\"conversations\"][2][\"value\"]\n",
    "\n",
    "        translated = model.translate([sys_val, input_text, response], source_lang=\"en\", target_lang='cs', perform_sentence_splitting=True, beam_size=15, batch_size=512)\n",
    "\n",
    "        # Create a new dictionary with the same format\n",
    "        translated_sample = {\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"system\", \"value\": translated[0]},\n",
    "                {\"from\": \"human\", \"value\": translated[1]},\n",
    "                {\"from\": \"gpt\", \"value\": translated[2]}\n",
    "            ]\n",
    "        }\n",
    "        # Write the lines to the file after each step\n",
    "        output_file.write(json.dumps(translated_sample, ensure_ascii=False))\n",
    "        output_file.write(\"\\n\")\n",
    "        output_file.flush()\n",
    "\n",
    "        # Print logs every 50 translated lines\n",
    "        if (i + 1) % 50 == 0:\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Translated {i + 1} lines in {elapsed_time:.2f} seconds.\")\n",
    "            start_time = end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ_se5Ey6GcH"
   },
   "source": [
    "# Available Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3Sk7Wwr6JjT"
   },
   "outputs": [],
   "source": [
    "available_models = ['opus-mt', 'mbart50_m2m', 'm2m_100_418M', \"m2m_100_1.2B\"]\n",
    "#Note: EasyNMT also provides the m2m_100_1.2B. But sadly it requires too much RAM to be loaded with the Colab free version here\n",
    "#If you start an empty instance in colab and load the 'm2m_100_1.2B' model, it should work.\n",
    "\n",
    "for model_name in available_models:\n",
    "  print(\"\\n\\nLoad model:\", model_name)\n",
    "  model = EasyNMT(model_name)\n",
    "\n",
    "  sentences = ['In dieser Liste definieren wir mehrere Sätze.',\n",
    "              'Jeder dieser Sätze wird dann in die Zielsprache übersetzt.',\n",
    "              'Puede especificar en esta lista la oración en varios idiomas.',\n",
    "              'El sistema detectará automáticamente el idioma y utilizará el modelo correcto.']\n",
    "  translations = model.translate(sentences, target_lang='en')\n",
    "\n",
    "  print(\"Translations:\")\n",
    "  for sent, trans in zip(sentences, translations):\n",
    "    print(sent)\n",
    "    print(\"=>\", trans, \"\\n\")\n",
    "  del model\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0517a0341e2b4bdaade5bbf0fddb5fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50bff47e4a424d2588460e39ab68c6ce",
      "placeholder": "​",
      "style": "IPY_MODEL_3cfb90d9325140f2bc4519c416087674",
      "value": "Creating json from Arrow format: 100%"
     }
    },
    "3cfb90d9325140f2bc4519c416087674": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50bff47e4a424d2588460e39ab68c6ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "567b92b20a3d454aa958708733d83f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6d28483343743b38ff2788a5bb346e9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d962b725c65d4250abc38021ab3b0ff3",
      "value": 1
     }
    },
    "82d187090d964bedbf0ca4002376cecf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6d28483343743b38ff2788a5bb346e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9da11d292e7441aaaf7725ae4d2508f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0517a0341e2b4bdaade5bbf0fddb5fca",
       "IPY_MODEL_567b92b20a3d454aa958708733d83f28",
       "IPY_MODEL_d6b15a0c4dee4b6a91ebb40d2247298b"
      ],
      "layout": "IPY_MODEL_82d187090d964bedbf0ca4002376cecf"
     }
    },
    "d6b15a0c4dee4b6a91ebb40d2247298b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9ad199c6fb2474ab8fd7915e8f4812f",
      "placeholder": "​",
      "style": "IPY_MODEL_e857d285567f4a6b9c293aa331077b21",
      "value": " 1/1 [00:00&lt;00:00, 29.83ba/s]"
     }
    },
    "d962b725c65d4250abc38021ab3b0ff3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e857d285567f4a6b9c293aa331077b21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9ad199c6fb2474ab8fd7915e8f4812f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
