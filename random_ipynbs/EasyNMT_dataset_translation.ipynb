{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9da11d292e7441aaaf7725ae4d2508f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0517a0341e2b4bdaade5bbf0fddb5fca",
              "IPY_MODEL_567b92b20a3d454aa958708733d83f28",
              "IPY_MODEL_d6b15a0c4dee4b6a91ebb40d2247298b"
            ],
            "layout": "IPY_MODEL_82d187090d964bedbf0ca4002376cecf"
          }
        },
        "0517a0341e2b4bdaade5bbf0fddb5fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50bff47e4a424d2588460e39ab68c6ce",
            "placeholder": "​",
            "style": "IPY_MODEL_3cfb90d9325140f2bc4519c416087674",
            "value": "Creating json from Arrow format: 100%"
          }
        },
        "567b92b20a3d454aa958708733d83f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6d28483343743b38ff2788a5bb346e9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d962b725c65d4250abc38021ab3b0ff3",
            "value": 1
          }
        },
        "d6b15a0c4dee4b6a91ebb40d2247298b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9ad199c6fb2474ab8fd7915e8f4812f",
            "placeholder": "​",
            "style": "IPY_MODEL_e857d285567f4a6b9c293aa331077b21",
            "value": " 1/1 [00:00&lt;00:00, 29.83ba/s]"
          }
        },
        "82d187090d964bedbf0ca4002376cecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50bff47e4a424d2588460e39ab68c6ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cfb90d9325140f2bc4519c416087674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6d28483343743b38ff2788a5bb346e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d962b725c65d4250abc38021ab3b0ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9ad199c6fb2474ab8fd7915e8f4812f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e857d285567f4a6b9c293aa331077b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvhuDvTUGJUF"
      },
      "source": [
        "#EasyNMT - Example (Opus-MT Model)\n",
        "This notebook shows the usage of [EasyNMT](https://github.com/UKPLab/EasyNMT) for machine translation.\n",
        "\n",
        "Here, we use the [Opus-MT model](https://github.com/Helsinki-NLP/Opus-MT). The Helsiniki-NLP group provides 1200+ pre-trained models for various language directions (e.g. en-de, es-fr, ru-fr). Each model has a size of about 300 MB.\n",
        "\n",
        "We make the usage of the models easy: The suitable model needed for your translation is loaded automatically and kept in memory for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc745ZyOIhKF",
        "outputId": "766abdcd-b09d-4830-d1ac-3d2e48102104"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 26 23:09:10 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean badboy"
      ],
      "metadata": {
        "id": "bo5irA2MZSsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run this to clean GPU memory\n",
        "import torch\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ZgJ2anS3ZSHx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KUU4nqKkXPD"
      },
      "source": [
        "!pip install -U easynmt\n",
        "!pip install -U datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "KlpteWMEVtJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh7kAqp3kjS2"
      },
      "source": [
        "from easynmt import EasyNMT\n",
        "model = EasyNMT('opus-mt')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\") # TODO: zahodit do pice lebo public repo xd, HF token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXaDCZ96N2-J",
        "outputId": "f360de1c-4152-4580-cc3d-945c54bb0d3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"Open-Orca/SlimOrca-Dedup\", split='train')\n",
        "\n",
        "# TODO process dataset?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5mJjGSY7ew1",
        "outputId": "61fed113-b6d8-4a4d-8e01-d29fe95dc98e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.to_json(\"dataset.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "a9da11d292e7441aaaf7725ae4d2508f",
            "0517a0341e2b4bdaade5bbf0fddb5fca",
            "567b92b20a3d454aa958708733d83f28",
            "d6b15a0c4dee4b6a91ebb40d2247298b",
            "82d187090d964bedbf0ca4002376cecf",
            "50bff47e4a424d2588460e39ab68c6ce",
            "3cfb90d9325140f2bc4519c416087674",
            "a6d28483343743b38ff2788a5bb346e9",
            "d962b725c65d4250abc38021ab3b0ff3",
            "e9ad199c6fb2474ab8fd7915e8f4812f",
            "e857d285567f4a6b9c293aa331077b21"
          ]
        },
        "id": "ypIAEdtsdgxs",
        "outputId": "f4bc0b66-1994-459c-8bd4-3cffda84b65f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9da11d292e7441aaaf7725ae4d2508f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174976"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# from transformers import MarianMTModel, MarianTokenizer\n",
        "# import torch\n",
        "# from typing import List\n",
        "\n",
        "\n",
        "# class OpusMT:\n",
        "#     def __init__(self, easynmt_path: str = None, max_loaded_models: int = 10):\n",
        "#         self.models = {}\n",
        "#         self.max_loaded_models = max_loaded_models\n",
        "#         self.max_length = None\n",
        "\n",
        "#     def load_model(self, model_name):\n",
        "#         if model_name in self.models:\n",
        "#             self.models[model_name]['last_loaded'] = time.time()\n",
        "#             return self.models[model_name]['tokenizer'], self.models[model_name]['model']\n",
        "#         else:\n",
        "#             tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "#             model = MarianMTModel.from_pretrained(model_name)\n",
        "#             model.eval()\n",
        "\n",
        "#             if len(self.models) >= self.max_loaded_models:\n",
        "#                 oldest_time = time.time()\n",
        "#                 oldest_model = None\n",
        "#                 for loaded_model_name in self.models:\n",
        "#                     if self.models[loaded_model_name]['last_loaded'] <= oldest_time:\n",
        "#                         oldest_model = loaded_model_name\n",
        "#                         oldest_time = self.models[loaded_model_name]['last_loaded']\n",
        "#                 del self.models[oldest_model]\n",
        "\n",
        "#             self.models[model_name] = {'tokenizer': tokenizer, 'model': model, 'last_loaded': time.time()}\n",
        "#             return tokenizer, model\n",
        "\n",
        "#     def translate_sentences(self, sentences: List[str], source_lang: str, target_lang: str, device: str, beam_size: int = 5, **kwargs):\n",
        "#         model_name = 'Helsinki-NLP/opus-mt-{}-{}'.format(source_lang, target_lang)\n",
        "#         tokenizer, model = self.load_model(model_name)\n",
        "#         model.to(device)\n",
        "\n",
        "#         inputs = tokenizer(sentences, truncation=True, padding=True, max_length=self.max_length, return_tensors=\"pt\")\n",
        "\n",
        "#         for key in inputs:\n",
        "#             inputs[key] = inputs[key].to(device)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             translated = model.generate(**inputs, num_beams=beam_size, **kwargs)\n",
        "#             # print((translated.shape))\n",
        "#             output = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "\n",
        "#         return output\n",
        "\n",
        "#     def save(self, output_path):\n",
        "#         return {\"max_loaded_models\": self.max_loaded_models}\n",
        "\n",
        "# opes_cz = OpusMT()\n",
        "# tokenizer_test, _ = opes_cz.load_model('Helsinki-NLP/opus-mt-en-cs')"
      ],
      "metadata": {
        "id": "yX1bHhnIUtUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB4i7NgGJvFm"
      },
      "source": [
        "# Document Translation\n",
        "You can also pass longer documents (or list of documents) to the `translate()` method.\n",
        "\n",
        "As Transformer models can only translate inputs up to 512 (or 1024) word pieces, we first perform sentence splitting. Then, each sentence is translated individually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKo8PaxtKHzP",
        "outputId": "3ddb300a-d7ea-408e-fcc9-cddeb705b39f"
      },
      "source": [
        "import tqdm\n",
        "document_1 = \"\"\"\n",
        "Berlin is the capital and largest city of Germany by both area and population.\n",
        "Its 3,769,495 inhabitants as of 31 December 2019 make it the most-populous city of the European Union, according to population within city limits.\n",
        "The city is also one of Germany's 16 federal states. It is surrounded by the state of Brandenburg, and contiguous with Potsdam, Brandenburg's capital.\n",
        "The two cities are at the center of the Berlin-Brandenburg capital region, which is, with about six million inhabitants and an area of more than 30,000 km2, Germany's third-largest metropolitan region after the Rhine-Ruhr and Rhine-Main regions.\n",
        "Berlin straddles the banks of the River Spree, which flows into the River Havel (a tributary of the River Elbe) in the western borough of Spandau.\n",
        "Among the city's main topographical features are the many lakes in the western and southeastern boroughs formed by the Spree, Havel, and Dahme rivers (the largest of which is Lake Müggelsee).\n",
        "\"\"\"\n",
        "# Due to its location in the European Plain, Berlin is influenced by a temperate seasonal climate.\n",
        "# About one-third of the city's area is composed of forests, parks, gardens, rivers, canals and lakes.\n",
        "# The city lies in the Central German dialect area, the Berlin dialect being a variant of the Lusatian-New Marchian dialects.\n",
        "\n",
        "# First documented in the 13th century and at the crossing of two important historic trade routes, Berlin became the capital of the Margraviate of Brandenburg (1417–1701), the Kingdom of Prussia (1701–1918), the German Empire (1871–1918), the Weimar Republic (1919–1933), and the Third Reich (1933–1945).\n",
        "# Berlin in the 1920s was the third-largest municipality in the world.\n",
        "# After World War II and its subsequent occupation by the victorious countries, the city was divided; West Berlin became a de facto West German exclave, surrounded by the Berlin Wall (1961–1989) and East German territory.\n",
        "# East Berlin was declared capital of East Germany, while Bonn became the West German capital.\n",
        "# Following German reunification in 1990, Berlin once again became the capital of all of Germany.\n",
        "\n",
        "# Berlin is a world city of culture, politics, media and science.\n",
        "# Its economy is based on high-tech firms and the service sector, encompassing a diverse range of creative industries, research facilities, media corporations and convention venues.\n",
        "# Berlin serves as a continental hub for air and rail traffic and has a highly complex public transportation network.\n",
        "# The metropolis is a popular tourist destination.\n",
        "# Significant industries also include IT, pharmaceuticals, biomedical engineering, clean tech, biotechnology, construction and electronics.\n",
        "# \"\"\"\n",
        "\n",
        "document_2 = \"\"\"\n",
        "Please add spaces between words: ThereweretheheelsofforeigninvasiontrampinguponFrance;therewasthedownfallofauEmpire,andthecaptivityofaBonaparte;andtheretheywerethemselves.\n",
        "\"\"\"\n",
        "print(tokenizer_test(document_2, truncation=True, padding=True, max_length=None, return_tensors=\"pt\")[\"input_ids\"].shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 52])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nebere to poslednu vetu ? HALO\n",
        "print(opes_cz.translate_sentences([document_2], source_lang=\"en\", target_lang=\"cs\", beam_size=20, device=\"cuda\"))"
      ],
      "metadata": {
        "id": "y_l5lEyVap6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Output:\")\n",
        "print(model.translate(document_2, target_lang='cs', perform_sentence_splitting=True, beam_size=15, batch_size=8))"
      ],
      "metadata": {
        "id": "Lfrjkf_dWHRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(150000 + 130 + 250 + 320, len(dataset)))"
      ],
      "metadata": {
        "id": "FicAOjtdr3v9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translate dataset\n",
        "- save to `translated_dataset.json`"
      ],
      "metadata": {
        "id": "1SLdm2TQbSTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# translate all samples from dataset\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/translated_dataset.json\"\n",
        "if os.path.exists(output_path):\n",
        "    os.remove(output_path)\n",
        "\n",
        "max_input_length = int(4096 / 4)\n",
        "truncated_count = 0\n",
        "\n",
        "print(f\"Translating dataset of size: {len(dataset)}\")\n",
        "with open(output_path, \"a\", encoding=\"utf-8\") as output_file:\n",
        "    start_time = time.time()\n",
        "    for i in range(len(dataset)):\n",
        "        sample = dataset[i]\n",
        "        sys_val = sample[\"conversations\"][0][\"value\"]\n",
        "        input_text = sample[\"conversations\"][1][\"value\"]\n",
        "        response = sample[\"conversations\"][2][\"value\"]\n",
        "\n",
        "        combined_input = f\"{sys_val} {input_text} {response}\"\n",
        "        if len(combined_input) <= max_input_length:\n",
        "            translated = model.translate([sys_val, input_text, response], source_lang=\"en\", target_lang='cs', perform_sentence_splitting=True, beam_size=15, batch_size=512)\n",
        "\n",
        "            # Create a new dictionary with the same format\n",
        "            translated_sample = {\n",
        "                \"conversations\": [\n",
        "                    {\"from\": \"system\", \"value\": translated[0]},\n",
        "                    {\"from\": \"human\", \"value\": translated[1]},\n",
        "                    {\"from\": \"gpt\", \"value\": translated[2]}\n",
        "                ]\n",
        "            }\n",
        "            # Write the lines to the file after each step\n",
        "            output_file.write(json.dumps(translated_sample, ensure_ascii=False))\n",
        "            output_file.write(\"\\n\")\n",
        "            output_file.flush()\n",
        "        else:\n",
        "            truncated_count += 1\n",
        "\n",
        "        # Print logs every 50 translated lines\n",
        "        if (i + 1) % 50 == 0:\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            print(f\"Translated {i + 1 - truncated_count} lines in {elapsed_time:.2f} seconds. Truncated {truncated_count} lines.\")\n",
        "            start_time = end_time\n",
        "\n",
        "print(f\"Finished translation. Truncated {truncated_count} lines due to length limits.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-55oKixQZen",
        "outputId": "8fe7df8b-cefd-4bd4-f831-a2c213141354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Translating dataset of size: 62661\n",
            "Translated 5 lines in 13.72 seconds. Truncated 45 lines.\n",
            "Translated 11 lines in 10.05 seconds. Truncated 89 lines.\n",
            "Translated 15 lines in 21.55 seconds. Truncated 135 lines.\n",
            "Translated 22 lines in 19.78 seconds. Truncated 178 lines.\n",
            "Translated 28 lines in 5.45 seconds. Truncated 222 lines.\n",
            "Translated 35 lines in 35.31 seconds. Truncated 265 lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ_se5Ey6GcH"
      },
      "source": [
        "# Available Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Sk7Wwr6JjT"
      },
      "source": [
        "available_models = ['opus-mt', 'mbart50_m2m', 'm2m_100_418M', \"m2m_100_1.2B\"]\n",
        "#Note: EasyNMT also provides the m2m_100_1.2B. But sadly it requires too much RAM to be loaded with the Colab free version here\n",
        "#If you start an empty instance in colab and load the 'm2m_100_1.2B' model, it should work.\n",
        "\n",
        "for model_name in available_models:\n",
        "  print(\"\\n\\nLoad model:\", model_name)\n",
        "  model = EasyNMT(model_name)\n",
        "\n",
        "  sentences = ['In dieser Liste definieren wir mehrere Sätze.',\n",
        "              'Jeder dieser Sätze wird dann in die Zielsprache übersetzt.',\n",
        "              'Puede especificar en esta lista la oración en varios idiomas.',\n",
        "              'El sistema detectará automáticamente el idioma y utilizará el modelo correcto.']\n",
        "  translations = model.translate(sentences, target_lang='en')\n",
        "\n",
        "  print(\"Translations:\")\n",
        "  for sent, trans in zip(sentences, translations):\n",
        "    print(sent)\n",
        "    print(\"=>\", trans, \"\\n\")\n",
        "  del model\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}