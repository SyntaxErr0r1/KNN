{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvhuDvTUGJUF"
   },
   "source": [
    "#EasyNMT - Example (Opus-MT Model)\n",
    "This notebook shows the usage of [EasyNMT](https://github.com/UKPLab/EasyNMT) for machine translation.\n",
    "\n",
    "Here, we use the [Opus-MT model](https://github.com/Helsinki-NLP/Opus-MT). The Helsiniki-NLP group provides 1200+ pre-trained models for various language directions (e.g. en-de, es-fr, ru-fr). Each model has a size of about 300 MB.\n",
    "\n",
    "We make the usage of the models easy: The suitable model needed for your translation is loaded automatically and kept in memory for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sc745ZyOIhKF",
    "outputId": "c8a2946b-51d8-42c5-81bc-1b05f3969ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 29 17:25:43 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:61:00.0 Off |                    0 |\n",
      "|  0%   63C    P0             70W /  300W |       0MiB /  46068MiB |      5%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo5irA2MZSsk"
   },
   "source": [
    "### Clean badboy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "ZgJ2anS3ZSHx",
    "outputId": "7081ff1d-b5b7-4d4d-fb71-1caca419c500"
   },
   "outputs": [],
   "source": [
    "# # Run this to clean GPU memory\n",
    "import torch\n",
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5KUU4nqKkXPD"
   },
   "outputs": [],
   "source": [
    "# !pip install -U easynmt\n",
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KlpteWMEVtJL"
   },
   "outputs": [],
   "source": [
    "# !pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qh7kAqp3kjS2"
   },
   "outputs": [],
   "source": [
    "from easynmt import EasyNMT\n",
    "model = EasyNMT('m2m_100_1.2B') # m2m_100_1.2B/opus-mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXaDCZ96N2-J",
    "outputId": "ed18d32b-36f1-4811-efc2-a994b3f3e6c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /storage/praha1/home/jurajdedic/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\") # TODO: zahodit do pice lebo public repo xd, HF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e5mJjGSY7ew1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15011\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split='train')\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "a9da11d292e7441aaaf7725ae4d2508f",
      "0517a0341e2b4bdaade5bbf0fddb5fca",
      "567b92b20a3d454aa958708733d83f28",
      "d6b15a0c4dee4b6a91ebb40d2247298b",
      "82d187090d964bedbf0ca4002376cecf",
      "50bff47e4a424d2588460e39ab68c6ce",
      "3cfb90d9325140f2bc4519c416087674",
      "a6d28483343743b38ff2788a5bb346e9",
      "d962b725c65d4250abc38021ab3b0ff3",
      "e9ad199c6fb2474ab8fd7915e8f4812f",
      "e857d285567f4a6b9c293aa331077b21"
     ]
    },
    "id": "ypIAEdtsdgxs",
    "outputId": "f4bc0b66-1994-459c-8bd4-3cffda84b65f"
   },
   "outputs": [],
   "source": [
    "# dataset.to_json(\"dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yX1bHhnIUtUd"
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "# import torch\n",
    "# from typing import List\n",
    "\n",
    "\n",
    "# class OpusMT:\n",
    "#     def __init__(self, easynmt_path: str = None, max_loaded_models: int = 10):\n",
    "#         self.models = {}\n",
    "#         self.max_loaded_models = max_loaded_models\n",
    "#         self.max_length = None\n",
    "\n",
    "#     def load_model(self, model_name):\n",
    "#         if model_name in self.models:\n",
    "#             self.models[model_name]['last_loaded'] = time.time()\n",
    "#             return self.models[model_name]['tokenizer'], self.models[model_name]['model']\n",
    "#         else:\n",
    "#             tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "#             model = MarianMTModel.from_pretrained(model_name)\n",
    "#             model.eval()\n",
    "\n",
    "#             if len(self.models) >= self.max_loaded_models:\n",
    "#                 oldest_time = time.time()\n",
    "#                 oldest_model = None\n",
    "#                 for loaded_model_name in self.models:\n",
    "#                     if self.models[loaded_model_name]['last_loaded'] <= oldest_time:\n",
    "#                         oldest_model = loaded_model_name\n",
    "#                         oldest_time = self.models[loaded_model_name]['last_loaded']\n",
    "#                 del self.models[oldest_model]\n",
    "\n",
    "#             self.models[model_name] = {'tokenizer': tokenizer, 'model': model, 'last_loaded': time.time()}\n",
    "#             return tokenizer, model\n",
    "\n",
    "#     def translate_sentences(self, sentences: List[str], source_lang: str, target_lang: str, device: str, beam_size: int = 5, **kwargs):\n",
    "#         model_name = 'Helsinki-NLP/opus-mt-{}-{}'.format(source_lang, target_lang)\n",
    "#         tokenizer, model = self.load_model(model_name)\n",
    "#         model.to(device)\n",
    "\n",
    "#         inputs = tokenizer(sentences, truncation=True, padding=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "#         for key in inputs:\n",
    "#             inputs[key] = inputs[key].to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             translated = model.generate(**inputs, num_beams=beam_size, **kwargs)\n",
    "#             # print((translated.shape))\n",
    "#             output = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "#         return output\n",
    "\n",
    "#     def save(self, output_path):\n",
    "#         return {\"max_loaded_models\": self.max_loaded_models}\n",
    "\n",
    "# opes_cz = OpusMT()\n",
    "# tokenizer_test, _ = opes_cz.load_model('Helsinki-NLP/opus-mt-en-cs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB4i7NgGJvFm"
   },
   "source": [
    "# Document Translation\n",
    "You can also pass longer documents (or list of documents) to the `translate()` method.\n",
    "\n",
    "As Transformer models can only translate inputs up to 512 (or 1024) word pieces, we first perform sentence splitting. Then, each sentence is translated individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKo8PaxtKHzP",
    "outputId": "3ddb300a-d7ea-408e-fcc9-cddeb705b39f"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "document_1 = \"\"\"\n",
    "Berlin is the capital and largest city of Germany by both area and population.\n",
    "Its 3,769,495 inhabitants as of 31 December 2019 make it the most-populous city of the European Union, according to population within city limits.\n",
    "The city is also one of Germany's 16 federal states. It is surrounded by the state of Brandenburg, and contiguous with Potsdam, Brandenburg's capital.\n",
    "The two cities are at the center of the Berlin-Brandenburg capital region, which is, with about six million inhabitants and an area of more than 30,000 km2, Germany's third-largest metropolitan region after the Rhine-Ruhr and Rhine-Main regions.\n",
    "Berlin straddles the banks of the River Spree, which flows into the River Havel (a tributary of the River Elbe) in the western borough of Spandau.\n",
    "Among the city's main topographical features are the many lakes in the western and southeastern boroughs formed by the Spree, Havel, and Dahme rivers (the largest of which is Lake Müggelsee).\n",
    "\"\"\"\n",
    "# Due to its location in the European Plain, Berlin is influenced by a temperate seasonal climate.\n",
    "# About one-third of the city's area is composed of forests, parks, gardens, rivers, canals and lakes.\n",
    "# The city lies in the Central German dialect area, the Berlin dialect being a variant of the Lusatian-New Marchian dialects.\n",
    "\n",
    "# First documented in the 13th century and at the crossing of two important historic trade routes, Berlin became the capital of the Margraviate of Brandenburg (1417–1701), the Kingdom of Prussia (1701–1918), the German Empire (1871–1918), the Weimar Republic (1919–1933), and the Third Reich (1933–1945).\n",
    "# Berlin in the 1920s was the third-largest municipality in the world.\n",
    "# After World War II and its subsequent occupation by the victorious countries, the city was divided; West Berlin became a de facto West German exclave, surrounded by the Berlin Wall (1961–1989) and East German territory.\n",
    "# East Berlin was declared capital of East Germany, while Bonn became the West German capital.\n",
    "# Following German reunification in 1990, Berlin once again became the capital of all of Germany.\n",
    "\n",
    "# Berlin is a world city of culture, politics, media and science.\n",
    "# Its economy is based on high-tech firms and the service sector, encompassing a diverse range of creative industries, research facilities, media corporations and convention venues.\n",
    "# Berlin serves as a continental hub for air and rail traffic and has a highly complex public transportation network.\n",
    "# The metropolis is a popular tourist destination.\n",
    "# Significant industries also include IT, pharmaceuticals, biomedical engineering, clean tech, biotechnology, construction and electronics.\n",
    "# \"\"\"\n",
    "\n",
    "document_2 = \"\"\"\n",
    "Please add spaces between words: ThereweretheheelsofforeigninvasiontrampinguponFrance;therewasthedownfallofauEmpire,andthecaptivityofaBonaparte;andtheretheywerethemselves.\n",
    "\"\"\"\n",
    "print(tokenizer_test(document_2, truncation=True, padding=True, max_length=None, return_tensors=\"pt\")[\"input_ids\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_l5lEyVap6B"
   },
   "outputs": [],
   "source": [
    "# nebere to poslednu vetu ? HALO\n",
    "print(opes_cz.translate_sentences([document_2], source_lang=\"en\", target_lang=\"cs\", beam_size=20, device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lfrjkf_dWHRy"
   },
   "outputs": [],
   "source": [
    "print(\"Output:\")\n",
    "print(model.translate(document_2, target_lang='cs', perform_sentence_splitting=True, beam_size=15, batch_size=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SLdm2TQbSTk"
   },
   "source": [
    "## Translate dataset\n",
    "- save to `translated_dataset.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5711\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.select(range(9300, len(dataset)))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "m-55oKixQZen",
    "outputId": "cf557918-0c69-4e54-b60a-9d9aecf924ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating dataset of size: 5711\n",
      "Translated 50 lines in 90.19 seconds. Truncated 0 lines.\n",
      "Translated 100 lines in 99.28 seconds. Truncated 0 lines.\n",
      "Translated 150 lines in 87.92 seconds. Truncated 0 lines.\n",
      "Translated 200 lines in 81.09 seconds. Truncated 0 lines.\n",
      "Translated 248 lines in 83.62 seconds. Truncated 2 lines.\n",
      "Translated 298 lines in 95.39 seconds. Truncated 2 lines.\n",
      "Translated 348 lines in 70.35 seconds. Truncated 2 lines.\n",
      "Translated 396 lines in 83.20 seconds. Truncated 4 lines.\n",
      "Translated 446 lines in 80.36 seconds. Truncated 4 lines.\n",
      "Translated 496 lines in 90.66 seconds. Truncated 4 lines.\n",
      "Translated 545 lines in 74.97 seconds. Truncated 5 lines.\n",
      "Translated 595 lines in 80.48 seconds. Truncated 5 lines.\n",
      "Translated 645 lines in 81.99 seconds. Truncated 5 lines.\n",
      "Translated 695 lines in 82.81 seconds. Truncated 5 lines.\n",
      "Translated 745 lines in 84.60 seconds. Truncated 5 lines.\n",
      "Translated 795 lines in 105.87 seconds. Truncated 5 lines.\n",
      "Translated 845 lines in 63.32 seconds. Truncated 5 lines.\n",
      "Translated 895 lines in 70.82 seconds. Truncated 5 lines.\n",
      "Translated 945 lines in 110.94 seconds. Truncated 5 lines.\n",
      "Translated 994 lines in 73.07 seconds. Truncated 6 lines.\n",
      "Translated 1044 lines in 91.09 seconds. Truncated 6 lines.\n",
      "Translated 1094 lines in 106.55 seconds. Truncated 6 lines.\n",
      "Translated 1144 lines in 89.47 seconds. Truncated 6 lines.\n",
      "Translated 1194 lines in 77.58 seconds. Truncated 6 lines.\n",
      "Translated 1244 lines in 70.15 seconds. Truncated 6 lines.\n",
      "Translated 1293 lines in 70.86 seconds. Truncated 7 lines.\n",
      "Translated 1343 lines in 86.07 seconds. Truncated 7 lines.\n",
      "Translated 1392 lines in 89.75 seconds. Truncated 8 lines.\n",
      "Translated 1442 lines in 66.35 seconds. Truncated 8 lines.\n",
      "Translated 1491 lines in 86.14 seconds. Truncated 9 lines.\n",
      "Translated 1540 lines in 91.90 seconds. Truncated 10 lines.\n",
      "Translated 1590 lines in 80.80 seconds. Truncated 10 lines.\n",
      "Translated 1640 lines in 97.81 seconds. Truncated 10 lines.\n",
      "Translated 1690 lines in 87.29 seconds. Truncated 10 lines.\n",
      "Translated 1740 lines in 90.32 seconds. Truncated 10 lines.\n",
      "Translated 1790 lines in 64.82 seconds. Truncated 10 lines.\n",
      "Translated 1840 lines in 92.09 seconds. Truncated 10 lines.\n",
      "Translated 1890 lines in 72.47 seconds. Truncated 10 lines.\n",
      "Translated 1940 lines in 79.13 seconds. Truncated 10 lines.\n",
      "Translated 1989 lines in 91.84 seconds. Truncated 11 lines.\n",
      "Translated 2039 lines in 107.92 seconds. Truncated 11 lines.\n",
      "Translated 2089 lines in 81.00 seconds. Truncated 11 lines.\n",
      "Translated 2138 lines in 113.65 seconds. Truncated 12 lines.\n",
      "Translated 2188 lines in 74.58 seconds. Truncated 12 lines.\n",
      "Translated 2238 lines in 89.60 seconds. Truncated 12 lines.\n",
      "Translated 2288 lines in 88.56 seconds. Truncated 12 lines.\n",
      "Translated 2338 lines in 91.96 seconds. Truncated 12 lines.\n",
      "Translated 2388 lines in 75.96 seconds. Truncated 12 lines.\n",
      "Translated 2437 lines in 86.37 seconds. Truncated 13 lines.\n",
      "Translated 2487 lines in 72.05 seconds. Truncated 13 lines.\n",
      "Translated 2537 lines in 83.77 seconds. Truncated 13 lines.\n",
      "Translated 2587 lines in 68.70 seconds. Truncated 13 lines.\n",
      "Translated 2637 lines in 74.99 seconds. Truncated 13 lines.\n",
      "Translated 2687 lines in 66.08 seconds. Truncated 13 lines.\n",
      "Translated 2737 lines in 72.56 seconds. Truncated 13 lines.\n",
      "Translated 2785 lines in 86.47 seconds. Truncated 15 lines.\n",
      "Translated 2835 lines in 89.04 seconds. Truncated 15 lines.\n",
      "Translated 2885 lines in 87.20 seconds. Truncated 15 lines.\n",
      "Translated 2935 lines in 99.13 seconds. Truncated 15 lines.\n",
      "Translated 2985 lines in 94.12 seconds. Truncated 15 lines.\n",
      "Translated 3034 lines in 82.19 seconds. Truncated 16 lines.\n",
      "Translated 3084 lines in 85.22 seconds. Truncated 16 lines.\n",
      "Translated 3134 lines in 87.12 seconds. Truncated 16 lines.\n",
      "Translated 3183 lines in 75.26 seconds. Truncated 17 lines.\n",
      "Translated 3233 lines in 86.79 seconds. Truncated 17 lines.\n",
      "Translated 3283 lines in 79.91 seconds. Truncated 17 lines.\n",
      "Translated 3333 lines in 90.64 seconds. Truncated 17 lines.\n",
      "Translated 3382 lines in 107.73 seconds. Truncated 18 lines.\n",
      "Translated 3431 lines in 83.03 seconds. Truncated 19 lines.\n",
      "Translated 3481 lines in 93.80 seconds. Truncated 19 lines.\n",
      "Translated 3531 lines in 91.98 seconds. Truncated 19 lines.\n",
      "Translated 3581 lines in 83.86 seconds. Truncated 19 lines.\n",
      "Translated 3631 lines in 91.06 seconds. Truncated 19 lines.\n",
      "Translated 3680 lines in 73.39 seconds. Truncated 20 lines.\n",
      "Translated 3730 lines in 94.14 seconds. Truncated 20 lines.\n",
      "Translated 3780 lines in 108.66 seconds. Truncated 20 lines.\n",
      "Translated 3830 lines in 72.14 seconds. Truncated 20 lines.\n",
      "Translated 3879 lines in 67.97 seconds. Truncated 21 lines.\n",
      "Translated 3929 lines in 88.97 seconds. Truncated 21 lines.\n",
      "Translated 3979 lines in 81.39 seconds. Truncated 21 lines.\n",
      "Translated 4029 lines in 86.47 seconds. Truncated 21 lines.\n",
      "Translated 4079 lines in 86.07 seconds. Truncated 21 lines.\n",
      "Translated 4128 lines in 74.62 seconds. Truncated 22 lines.\n",
      "Translated 4178 lines in 92.39 seconds. Truncated 22 lines.\n",
      "Translated 4228 lines in 86.04 seconds. Truncated 22 lines.\n",
      "Translated 4276 lines in 64.06 seconds. Truncated 24 lines.\n",
      "Translated 4326 lines in 80.57 seconds. Truncated 24 lines.\n",
      "Translated 4376 lines in 104.95 seconds. Truncated 24 lines.\n",
      "Translated 4426 lines in 86.42 seconds. Truncated 24 lines.\n",
      "Translated 4475 lines in 69.04 seconds. Truncated 25 lines.\n",
      "Translated 4525 lines in 102.19 seconds. Truncated 25 lines.\n",
      "Translated 4575 lines in 75.87 seconds. Truncated 25 lines.\n",
      "Translated 4624 lines in 76.92 seconds. Truncated 26 lines.\n",
      "Translated 4674 lines in 91.16 seconds. Truncated 26 lines.\n",
      "Translated 4723 lines in 90.92 seconds. Truncated 27 lines.\n",
      "Translated 4773 lines in 92.30 seconds. Truncated 27 lines.\n",
      "Translated 4823 lines in 69.82 seconds. Truncated 27 lines.\n",
      "Translated 4871 lines in 102.56 seconds. Truncated 29 lines.\n",
      "Translated 4921 lines in 104.57 seconds. Truncated 29 lines.\n",
      "Translated 4971 lines in 74.55 seconds. Truncated 29 lines.\n",
      "Translated 5021 lines in 85.31 seconds. Truncated 29 lines.\n",
      "Translated 5071 lines in 68.28 seconds. Truncated 29 lines.\n",
      "Translated 5121 lines in 73.12 seconds. Truncated 29 lines.\n",
      "Translated 5171 lines in 75.91 seconds. Truncated 29 lines.\n",
      "Translated 5221 lines in 65.54 seconds. Truncated 29 lines.\n",
      "Translated 5271 lines in 92.67 seconds. Truncated 29 lines.\n",
      "Translated 5321 lines in 63.85 seconds. Truncated 29 lines.\n",
      "Translated 5371 lines in 89.40 seconds. Truncated 29 lines.\n",
      "Translated 5421 lines in 83.36 seconds. Truncated 29 lines.\n",
      "Translated 5471 lines in 83.62 seconds. Truncated 29 lines.\n",
      "Translated 5521 lines in 93.39 seconds. Truncated 29 lines.\n",
      "Translated 5571 lines in 77.24 seconds. Truncated 29 lines.\n",
      "Translated 5621 lines in 89.17 seconds. Truncated 29 lines.\n",
      "Translated 5671 lines in 71.95 seconds. Truncated 29 lines.\n",
      "Finished translation. Truncated 29 lines due to length limits.\n"
     ]
    }
   ],
   "source": [
    "# translate all samples from dataset\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "output_path = \"translated_dataset_dolly-databricks.jsonl\"\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "max_input_length = 8192\n",
    "truncated_count = 0\n",
    "\n",
    "print(f\"Translating dataset of size: {len(dataset)}\")\n",
    "with open(output_path, \"a\", encoding=\"utf-8\") as output_file:\n",
    "    start_time = time.time()\n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]\n",
    "        instruction = sample[\"instruction\"]\n",
    "        context = sample[\"context\"]\n",
    "        response = sample[\"response\"]\n",
    "\n",
    "        combined_input = f\"{instruction} {context} {response}\"\n",
    "        if len(combined_input) <= max_input_length:\n",
    "            translated = model.translate([instruction, context, response], source_lang=\"en\", target_lang='cs', perform_sentence_splitting=True, beam_size=10, batch_size=8)\n",
    "\n",
    "            # Create a new dictionary with the same format\n",
    "            translated_sample = {\n",
    "                    \"instruction\": translated[0],\n",
    "                    \"context\": translated[1],\n",
    "                    \"response\": translated[2],\n",
    "                    \"category\": sample[\"category\"]\n",
    "            }\n",
    "            # Write the lines to the file after each step\n",
    "            output_file.write(json.dumps(translated_sample, ensure_ascii=False))\n",
    "            output_file.write(\"\\n\")\n",
    "            output_file.flush()\n",
    "        else:\n",
    "            truncated_count += 1\n",
    "\n",
    "        # Print logs every 50 translated lines\n",
    "        if (i + 1) % 50 == 0:\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Translated {i + 1 - truncated_count} lines in {elapsed_time:.2f} seconds. Truncated {truncated_count} lines.\")\n",
    "            start_time = end_time\n",
    "\n",
    "print(f\"Finished translation. Truncated {truncated_count} lines due to length limits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ_se5Ey6GcH"
   },
   "source": [
    "# Available Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3Sk7Wwr6JjT"
   },
   "outputs": [],
   "source": [
    "available_models = ['opus-mt', 'mbart50_m2m', 'm2m_100_418M', \"m2m_100_1.2B\"]\n",
    "#Note: EasyNMT also provides the m2m_100_1.2B. But sadly it requires too much RAM to be loaded with the Colab free version here\n",
    "#If you start an empty instance in colab and load the 'm2m_100_1.2B' model, it should work.\n",
    "\n",
    "for model_name in available_models:\n",
    "  print(\"\\n\\nLoad model:\", model_name)\n",
    "  model = EasyNMT(model_name)\n",
    "\n",
    "  sentences = ['In dieser Liste definieren wir mehrere Sätze.',\n",
    "              'Jeder dieser Sätze wird dann in die Zielsprache übersetzt.',\n",
    "              'Puede especificar en esta lista la oración en varios idiomas.',\n",
    "              'El sistema detectará automáticamente el idioma y utilizará el modelo correcto.']\n",
    "  translations = model.translate(sentences, target_lang='en')\n",
    "\n",
    "  print(\"Translations:\")\n",
    "  for sent, trans in zip(sentences, translations):\n",
    "    print(sent)\n",
    "    print(\"=>\", trans, \"\\n\")\n",
    "  del model\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0517a0341e2b4bdaade5bbf0fddb5fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50bff47e4a424d2588460e39ab68c6ce",
      "placeholder": "​",
      "style": "IPY_MODEL_3cfb90d9325140f2bc4519c416087674",
      "value": "Creating json from Arrow format: 100%"
     }
    },
    "3cfb90d9325140f2bc4519c416087674": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50bff47e4a424d2588460e39ab68c6ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "567b92b20a3d454aa958708733d83f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6d28483343743b38ff2788a5bb346e9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d962b725c65d4250abc38021ab3b0ff3",
      "value": 1
     }
    },
    "82d187090d964bedbf0ca4002376cecf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6d28483343743b38ff2788a5bb346e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9da11d292e7441aaaf7725ae4d2508f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0517a0341e2b4bdaade5bbf0fddb5fca",
       "IPY_MODEL_567b92b20a3d454aa958708733d83f28",
       "IPY_MODEL_d6b15a0c4dee4b6a91ebb40d2247298b"
      ],
      "layout": "IPY_MODEL_82d187090d964bedbf0ca4002376cecf"
     }
    },
    "d6b15a0c4dee4b6a91ebb40d2247298b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9ad199c6fb2474ab8fd7915e8f4812f",
      "placeholder": "​",
      "style": "IPY_MODEL_e857d285567f4a6b9c293aa331077b21",
      "value": " 1/1 [00:00&lt;00:00, 29.83ba/s]"
     }
    },
    "d962b725c65d4250abc38021ab3b0ff3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e857d285567f4a6b9c293aa331077b21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9ad199c6fb2474ab8fd7915e8f4812f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
