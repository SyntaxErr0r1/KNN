{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynks1jlffxZe",
    "outputId": "10ce83f4-d401-461e-fcbc-a338fc23c48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 30 12:10:23 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:61:00.0 Off |                    0 |\n",
      "|  0%   42C    P0             38W /  300W |      31MiB /  46068MiB |      5%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets \n",
    "# !pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "# !pip install peft\n",
    "# !pip install trl\n",
    "# !pip install transformers[torch] -U\n",
    "# !pip install accelerate -U\n",
    "# !pip install bitsandbytes\n",
    "\n",
    "# # be sure to install right flash-attn, we use torch compiled with CUDA 12.1, no ABI, python 3.9, Linux x86_64 architecture\n",
    "# !pip install flash-attn -U --no-build-isolation\n",
    " \n",
    "# !pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.3/flash_attn-2.5.3+cu122torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl --no-build-isolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this to clean GPU memory\n",
    "import torch\n",
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EVTNvJG7fxZf"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "from accelerate import Accelerator\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset & configure templates\n",
    "\n",
    "- *Can be skipped if dataset already on disk* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_gux7ajYfxZf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18408 14973\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset(\"json\",name=\"SumeCzech\", data_files=\"./sumeczech/sumeczech-1.0-dev.jsonl\", split=\"train\", num_proc=64)\n",
    "dataset_openorca = load_dataset(\"json\",name=\"TranslatedInstruct\", data_files=\"./datasets/translated_dataset_open-orca.jsonl\", split=\"train\")\n",
    "dataset_dolly = load_dataset(\"json\",name=\"TranslatedDolly\", data_files=\"./datasets/translated_dataset_dolly-databricks.jsonl\", split=\"train\")\n",
    "dataset_openorca = dataset_openorca.shuffle(seed=69)\n",
    "dataset_dolly = dataset_dolly.shuffle(seed=69)\n",
    "print(len(dataset_openorca), len(dataset_dolly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EctDCMg0fxZf"
   },
   "outputs": [],
   "source": [
    "# use only first 1000 examples\n",
    "# dataset = dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z5Vj3tqXfxZg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def formatting_prompts_func(example, in_type, out_type):\n",
    "    \"\"\"\n",
    "    Prepare the input text for the model\n",
    "    in_type: [\"abstract\", \"text\"] what to summarize from\n",
    "    out_type: [\"abstract\",\"headline\"] what to summarize to\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        in_text = example[in_type]\n",
    "        out_text = example[out_type]\n",
    "    except:\n",
    "        print(\"ERROR\")\n",
    "        print(example)\n",
    "\n",
    "    def param_type_to_czech(param_type):\n",
    "        if param_type == \"abstract\":\n",
    "            return \"abstrakt\"\n",
    "        elif param_type == \"headline\":\n",
    "            return \"nadpis\"\n",
    "        else:\n",
    "            return param_type\n",
    "\n",
    "    task_instructions = {\n",
    "        (\"text\", \"abstract\"): [\n",
    "            \"Sumarizuj vstupní {in_type} na stručný {out_type}.\",\n",
    "            \"Vytvoř výstižný {out_type} na základě zadaného {in_type}.\",\n",
    "            \"Přeformuluj zadaný {in_type} do stručného {out_type}.\",\n",
    "            \"Zpracuj vstupní {in_type} a napiš výstižný {out_type}.\",\n",
    "            \"Shrň podstatné informace ze vstupního {in_type} do {out_type}.\"\n",
    "        ],\n",
    "        (\"text\", \"headline\"): [\n",
    "            \"Sumarizuj vstupní {in_type} na výstižný {out_type}.\",\n",
    "            \"Vytvoř poutavý {out_type} na základě zadaného {in_type}.\",\n",
    "            \"Přeformuluj zadaný {in_type} do stručného {out_type}.\",\n",
    "            \"Zpracuj vstupní {in_type} a napiš výstižný {out_type}.\",\n",
    "            \"Shrň podstatné informace ze vstupního {in_type} do poutavého {out_type}.\"\n",
    "        ],\n",
    "        (\"abstract\", \"headline\"): [\n",
    "            \"Sumarizuj vstupní {in_type} na výstižný {out_type}.\",\n",
    "            \"Vytvoř poutavý {out_type} na základě zadaného {in_type}.\",\n",
    "            \"Přeformuluj zadaný {in_type} do stručného {out_type}.\",\n",
    "            \"Zpracuj vstupní {in_type} a napiš výstižný {out_type}.\",\n",
    "            \"Shrň podstatné informace ze vstupního {in_type} do poutavého {out_type}.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    task_instruction = random.choice(task_instructions[(in_type, out_type)])\n",
    "\n",
    "    text = f\"\"\"### Instrukce:\n",
    "{task_instruction}\n",
    "\n",
    "### Vstup:\n",
    "{in_text}\n",
    "\n",
    "### Výstup:\n",
    "{out_text}\n",
    "<|endoftext|>[EOS]\"\"\"\n",
    "\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for sumeczech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_to_abstract = copy.deepcopy(dataset)\n",
    "dataset_abstract_to_headline = copy.deepcopy(dataset)\n",
    "dataset_text_to_headline = copy.deepcopy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "krbEHZZ5qCKx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8dc2bc97354025b5381cc190970ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/44567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54351dd55514afcbe17d193ad9ff350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/44567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0b4708c1af43c6b2e05ef159224339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/44567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transform dataset so it has only field \"text\" with formatted prompts\n",
    "dataset_text_to_abstract = dataset_text_to_abstract.map(\n",
    "    formatting_prompts_func,\n",
    "    remove_columns=dataset_text_to_abstract.column_names,\n",
    "    num_proc=64,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"in_type\":\"text\", \"out_type\": \"abstract\"}\n",
    ")\n",
    "dataset_abstract_to_headline = dataset_abstract_to_headline.map(\n",
    "    formatting_prompts_func,\n",
    "    remove_columns=dataset_abstract_to_headline.column_names,\n",
    "    num_proc=64,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"in_type\":\"abstract\", \"out_type\": \"headline\"}\n",
    ")\n",
    "dataset_text_to_headline = dataset_text_to_headline.map(\n",
    "    formatting_prompts_func,\n",
    "    remove_columns=dataset_text_to_headline.column_names,\n",
    "    num_proc=64,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"in_type\":\"text\", \"out_type\": \"headline\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_text_to_abstract[69][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVDEjeKTqFfg",
    "outputId": "ca33de5b-88c0-45a1-8e44-4adf54e766b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instrukce:\n",
      "Zpracuj vstupní {in_type} a napiš výstižný {out_type}.\n",
      "\n",
      "### Vstup:\n",
      "S kolty proklatě nízko u pasu se můžete prohánět ve westernové takticko-akční střílečce Desperados: Cooper\"s Revenge. A nyní navíc pěkně v češtině.\n",
      "\n",
      "### Výstup:\n",
      "Desperados II: Cooper\"s Revenge\n",
      "<|endoftext|>[EOS]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_abstract_to_headline[69][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instrukce:\n",
      "Přeformuluj zadaný {in_type} do stručného {out_type}.\n",
      "\n",
      "### Vstup:\n",
      "Jelikož jsem náruživým fanouškem jakýchkoliv taktických her, nemohl jsem\n",
      "minout tento skvost, jež je pokračovatelem prvního dílu Desperados:\n",
      "Wanted Dead or Alive. Ihned po jejím spuštěním mně bylo jasné, tohle je\n",
      "hra mých snů a hru snů přece nebudu hrát v cizím jazyce. Pustil jsem se\n",
      "do díla a o několik dnů později předkládám hotovou češtinu k této úžasné\n",
      "hře. V tomto pokračování se chopíte všech známých postav z minulého dílu\n",
      "hry a tedy Coopera, Doktora McCoye, Kate O\"Hary, Sama, Sancheze a\n",
      "Indiána Hawkeyho. Projdete s nimi 14 obsáhlých levelů vyprávějící příběh\n",
      "Cooperovi pomsty za vraždu svého bratra.\n",
      "Instalace je jednoduchá. Řiďte se pokyny v instalátoru. V případě\n",
      "jakýchkoliv problémů se spuštěním nebo hraním počeštěné hry se můžete\n",
      "ozvat do diskuze nebo na můj email. Stejně tak, najdete-li nějaké\n",
      "nesrovnalosti, například text nekorespondující se skutečným děním ve hře\n",
      "(samozřejmě se to může stát, protože kontrola překladu byla dost\n",
      "obtížná). Budu vděčný za každý názor.\n",
      "4.6.2006 - Protože vyšel na hru patch 1.01, mým důležitým úkolem bylo zkontrolovat češtinu a její funkčnost s tímto patchem. Vzhledem k tomu, že patch nemění texty ve hře, je všechno OK a čeština funguje i s ním. Jen musíte po instalaci patche češtinu ještě jednou nainstalovat.\n",
      "Upozornění\n",
      "Tato čeština nesmí být bez souhlasu autorů zveřejněna jinde než na serveru Cestiny.CZ. Pokud chcete češtinu šířit dále, tak jedině odkazem na tuto stránku. Dále nesmí být zmíněný patch bez souhlasu autorů upravován nebo jinak měněn.\n",
      "Download češtiny do hry Desperados II: Cooper\"s Revenge (398 kB)\n",
      "Další informace o hře naleznete na serveru BonusWeb.\n",
      "Domovská stránka autora překladu, Jakuba Machaly, díky za návštěvu.\n",
      "Zde si můžete prohlédnout další obrázky z přeložené části\n",
      "\n",
      "### Výstup:\n",
      "Desperados II: Cooper\"s Revenge\n",
      "<|endoftext|>[EOS]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_text_to_headline[69][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "dataset_new = concatenate_datasets([dataset_text_to_abstract, dataset_abstract_to_headline, dataset_text_to_headline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39a265405144c6fb2ebd93702464fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/64 shards):   0%|          | 0/133701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_new.save_to_disk(\"datasets/sumeczech-full-3t-prompt-format\", num_proc=64)\n",
    "# TODO: filter to max 2k sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for instruct translated: openorca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    \"\"\"\n",
    "    Prepare the input text for the model\n",
    "    \"\"\"\n",
    "    sys_val = example[\"conversations\"][0][\"value\"]\n",
    "    input_text = example[\"conversations\"][1][\"value\"]\n",
    "    response = example[\"conversations\"][2][\"value\"]\n",
    "#     in_text=\"###MISSING###\"\n",
    "#     out_text=\"###MISSING###\"\n",
    "    \n",
    "#     try:\n",
    "#         in_text = example['abstract']\n",
    "#         out_text = example['headline']\n",
    "#     except:\n",
    "# #         print(\"hehe something went wrong\")\n",
    "#         pass\n",
    "    return {\"text\": \n",
    "f\"\"\"### System:\n",
    "{sys_val}\n",
    "\n",
    "### Uživatel:\n",
    "{input_text}\n",
    "\n",
    "### Asistent:\n",
    "{response}<|endoftext|>[EOS]\"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68567121efcf49c3a2c82f6819c16200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/18408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transform dataset so it has only field \"text\" with formatted prompts\n",
    "dataset_openorca = dataset_openorca.map(\n",
    "    formatting_prompts_func,\n",
    "    remove_columns=dataset_openorca.column_names,\n",
    "    num_proc=64,\n",
    "    batched=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System:\n",
      "Jsi asistentka AI. Uživatel vám dá úkol. Vaším cílem je dokončit úkol tak věrně, jak můžete. Při plnění úkolu myslet krok za krokem a ospravedlnit své kroky.\n",
      "\n",
      "### Uživatel:\n",
      "Pokračujte v psaní následujícího textu.\n",
      "\n",
      "Samantha byla schopna úspěšně množit a prodávat několik párů hrdliček, ale Sarah nemohla, protože \n",
      "Vyberte si z: [-] Samantha byla nekvalifikovaná na manipulaci se zvířaty.Sarah nebyla schopná zacházet se zvířaty.;\n",
      "\n",
      "### Asistent:\n",
      "Samantha byla schopna úspěšně množit a prodávat několik párů hrdliček, ale Sarah nemohla, protože Sarah byla nekvalifikovaná v zacházení se zvířaty.\n",
      "\n",
      "Krok 1: Přečtěte si daný text a pochopte kontext.\n",
      "V této souvislosti je Samantha schopna chovat hrdličky a prodávat je, zatímco Sarah není schopna.\n",
      "\n",
      "Krok 2: Analyzovat možnosti\n",
      "Možnost 1: [-] Samantha byla nekvalifikovaná při manipulaci se zvířaty.\n",
      "Možnost 2: [-] Sára nebyla schopná zacházet se zvířaty.\n",
      "\n",
      "Krok 3: Zvolte vhodnou volbu na základě kontextu\n",
      "Vzhledem k tomu, Samantha byla schopna úspěšně chovat a prodávat hrdličky, je nepravděpodobné, že by byla nekvalifikovaná v zacházení se zvířaty. Na druhou stranu, Sarah nemohla dosáhnout stejného úspěchu, takže je věrohodnější, že je to ona, kdo je nekvalifikovaný v zacházení se zvířaty.\n",
      "\n",
      "Krok 4: Dokončit text zvolenou volbou\n",
      "Volba volby 2, můžeme dokončit text takto:\n",
      "\"Samantha dokázala úspěšně rozmnožovat a prodávat několik párů hrdliček, ale Sarah nemohla, protože Sarah nebyla schopná zacházet se zvířaty.\"<|endoftext|>[EOS]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_openorca[421][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for instruct translated: dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    \"\"\"\n",
    "    Prepare the input text for the model\n",
    "    \"\"\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    context = example.get(\"context\", \"\")  # Handle missing context\n",
    "    response = example[\"response\"]\n",
    "    category = example[\"category\"]\n",
    "    \n",
    "    prompt = \"Odpovězte na danou instrukci na základě poskytnutého kontextu (pokud nějaký je).\\n\\n\"\n",
    "    prompt += f\"### Instrukce: \\n{instruction}\\n\"\n",
    "    \n",
    "    if context:\n",
    "        prompt += f\"### Kontext:\\n{context}\\n\"\n",
    "    \n",
    "    prompt += f\"### Odpoved:\\n{response}\"\n",
    "    prompt += f\"<|endoftext|>[EOS]\"\n",
    "    \n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968b8c4a1b7e44619cdebdb86ec28594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/14973 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transform dataset so it has only field \"text\" with formatted prompts\n",
    "dataset_dolly = dataset_dolly.map(\n",
    "    formatting_prompts_func,\n",
    "    remove_columns=dataset_dolly.column_names,\n",
    "    num_proc=64,\n",
    "    batched=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpovězte na danou instrukci na základě poskytnutého kontextu (pokud nějaký je).\n",
      "\n",
      "### Instrukce: \n",
      "Jaké součásti reprodukce zvuku jsou důležité v domácím prostředí?\n",
      "### Odpoved:\n",
      "Chcete-li přesně reprodukovat zvuk doma, důležité věci, na které se zaměřit jsou umístění reproduktorů, umístění posluchače, tvar místnosti a materiály uvnitř místnosti. Například umístění reproduktorů by mělo být v souladu s pokyny pro nastavení, například stereo reproduktory budou umístěny ve stejném úhlu a výšce k posluchači, umístěné několik stop od jakýchkoli stěn. Tvar místnosti a materiály definují, jak se zvuk odráží v prostoru, s cílem snížit překrývající se echo z plochých povrchů.<|endoftext|>[EOS]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dolly[424][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved to disk for easier loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efa43523d754f27b6d31673b7c475ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14973 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dolly.save_to_disk(\"./datasets/dolly-databricks_format\")\n",
    "dataset_openorca.save_to_disk(\"./datasets/open-orca_format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & configure training \n",
    "- load model into the GPU\n",
    "- confogire LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14973\n",
      "18408\n"
     ]
    }
   ],
   "source": [
    "dataset_dolly = load_from_disk(\"./datasets/dolly-databricks_format\")\n",
    "print(len(dataset_dolly))\n",
    "\n",
    "dataset_openorca = load_from_disk(\"./datasets/open-orca_format\")\n",
    "print(len(dataset_openorca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33381\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "dataset_new = concatenate_datasets([dataset_dolly, dataset_openorca])\n",
    "print(len(dataset_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pm9gHGlefxZg",
    "outputId": "764b8537-5e09-46f1-d8d3-8118af1219c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /storage/praha1/home/xhorni20/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"BUT-FIT/CSTinyLlama-1.2B\"\n",
    "new_model_path = \"CSTinyLlama-1.2B-Instruct\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\") # HF token TODO: zahodit do pice lebo public repo xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198,
     "referenced_widgets": [
      "d3dd5ef4623e406daa5d2f019570f3b2",
      "8f26b29ee13c4f6e95e8cd723410718f",
      "d0f505d1cc81485989225fb328bc396a",
      "7486ae8153d444a6b57df2ca840490ee",
      "cb2a038fcda44e4da4d89e3767fa8911",
      "67f7b7125a4e42aabdd3614b3f8a079d",
      "046b7d5b13104068b60e0b67f7c8313d",
      "5ba881790f914e77ab171d906abc9d8d",
      "4bd047b485aa4e6d86cd4cda101cc98f",
      "8f8abfefc24942f4ad5fd62262b06168",
      "2ea80c14f1314b5c95fa997b67420dc9"
     ]
    },
    "id": "OiObI35wfxZg",
    "outputId": "21a673c1-c128-4dbb-ab07-4a7e35d85d30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, '[EOS]')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    base_model_name, \n",
    "    trust_remote_code=True,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "\n",
    "model.config.pretraining_tp = 1\n",
    "# model.gradient_checkpointing_enable()\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the dataset using tokenizer\n",
    "\n",
    "- *Can be skipped if dataset already on disk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_length(example):\n",
    "    text = example['text']\n",
    "    \n",
    "    text_ids = tokenizer.encode(text, return_tensors=\"pt\",padding=True)\n",
    "    \n",
    "    encoded_length = text_ids.shape[1]\n",
    "    \n",
    "    if encoded_length < (2048):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset samples before filtering: 33381\n",
      "Dataset samples after filtering: 33369\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset samples before filtering:\", len(dataset_new))\n",
    "dataset_new = dataset_new.filter(is_good_length, num_proc=64)\n",
    "print(\"Dataset samples after filtering:\", len(dataset_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'v_proj',\n",
       " 'down_proj',\n",
       " 'gate_proj',\n",
       " 'k_proj',\n",
       " 'o_proj',\n",
       " 'up_proj',\n",
       " 'q_proj']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Conv1D\n",
    "\n",
    "def get_specific_layer_names(model):\n",
    "    # Create a list to store the layer names\n",
    "    layer_names = []\n",
    "    \n",
    "    # Recursively visit all modules and submodules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is an instance of the specified layers\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
    "            # model name parsing \n",
    "\n",
    "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
    "    \n",
    "    return layer_names\n",
    "\n",
    "list(set(get_specific_layer_names(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(64002, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=64002, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NHAZ2UW1fxZg"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128, # TODO: Mozno zmenit\n",
    "    lora_dropout=0.02,\n",
    "    r=64, # TODO: Mozno zmenit\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"gate_proj\",\"k_proj\",\"v_proj\", \"q_proj\",\"o_proj\", \"up_proj\", \"down_proj\"] #TODO: wte maybe\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161,
     "referenced_widgets": [
      "6be2100c4c7f4605a4ed20d59627a830",
      "754e9f27204242f09fa7a3f59373512e",
      "71bf17a0908d48b4ad8e444ea014398f",
      "5d993c8164da4d42b1d1a00a171dd483",
      "fc0a449394a4422abb814f4f1e8d9319",
      "9745305d1cf6400e985450fcee4cca12",
      "d3bb4824364e485aa81859e4762df7a2",
      "7cfc37e675eb420590003bdd30eef482",
      "16a9c451d8424877ba84bd021334e9ee",
      "45c95b08e74845af90fe05f1ae826e14",
      "234bc12a982c41ae93b93156856c3a81"
     ]
    },
    "id": "umJcFMBPfxZh",
    "outputId": "2e18393e-a5f5-4591-eb4d-ed6c3ddc8ef5"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=6, # TODO: uvidime kolko bude stacit\n",
    "    per_device_train_batch_size=4, # TODO: mozno zmenit\n",
    "    gradient_accumulation_steps=4, # TODO: mozno zmenit\n",
    "    optim=\"paged_adamw_32bit\", # adamw_torch_fused\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=25,\n",
    "    learning_rate=6e-5,\n",
    "    weight_decay=0.000025,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.06,\n",
    "    group_by_length=False, #could cause the oscillation in loss (https://github.com/artidoro/qlora/issues/228)\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_new,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= 2048, # maximum pre BUT Tiny LLama\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "#     neftune_noise_alpha=5, #should improve the performance but needs to be tested\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "71iKvh-zfxZh",
    "outputId": "bfc19a89-3f80-4591-fed3-bb22c7bb7410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 50462720 || all params: 1281591296 || trainable%: 3.9375048939158837\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3176' max='12510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3176/12510 56:45 < 2:46:54, 0.93 it/s, Epoch 1.52/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.787300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.588600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.419100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.363400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.388400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.409600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.305200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.323700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>2.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>2.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>2.328300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>2.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>2.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>2.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>2.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>2.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>2.246100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>2.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>2.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>2.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>2.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>2.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>2.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>2.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>2.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>2.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>2.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>2.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>2.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>2.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>2.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>2.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>2.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>2.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>2.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>2.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>2.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>2.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>2.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>2.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>2.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>2.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>2.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>2.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>2.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>2.173200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>2.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>2.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>2.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>2.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>2.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>2.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>2.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>2.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3025</td>\n",
       "      <td>2.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3075</td>\n",
       "      <td>2.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3125</td>\n",
       "      <td>2.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.186200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(trainer.model)\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVYR0Gv0fxZh",
    "outputId": "d4525002-e110-45ef-a81e-57ee3dcd251f"
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model_path)\n",
    "trainer.tokenizer.save_pretrained(new_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for inference\n",
    "\n",
    "- build prompt\n",
    "- load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285459a5a70c48918b93f03d1c7ec931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129356\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_from_disk(\"datasets/sumeczech_test-40k-3t-prompt-format-filter_1.5kseq\")\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /storage/praha1/home/xhorni20/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"BUT-FIT/CSTinyLlama-1.2B\"\n",
    "new_model_path = \"CSTinyLlama-1.2B-Instruct\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\") # HF token TODO: zahodit do pice lebo public repo xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "base_model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(64002, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=64002, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig.from_pretrained(new_model_path)\n",
    "new_model = get_peft_model(base_model, lora_config)\n",
    "new_model.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer = AutoTokenizer.from_pretrained(new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VrYWMvktfxZi"
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# pipe = pipeline(task=\"text-generation\", model=new_model, tokenizer=new_tokenizer, torch_dtype=torch.bfloat16, device_map=\"auto\", max_length=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_raw = test_dataset[100002][\"text\"]\n",
    "example = example_raw.split(\"### Výstup:\")[0] + \"### Výstup:\"\n",
    "print(example)\n",
    "# print(example_raw.split(\"### Výstup:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    pipe.tokenizer.eos_token_id,\n",
    "    pipe.tokenizer.convert_tokens_to_ids(\".\"),\n",
    "    pipe.tokenizer.convert_tokens_to_ids(\"<|endoftext|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_as_list(word_list):\n",
    "    \"Converts a sequence of words into a list of tokens\"\n",
    "    tokens_list = []\n",
    "    for word in word_list:\n",
    "        tokenized_word = new_tokenizer([word], add_special_tokens=False).input_ids[0]\n",
    "        tokens_list.append(tokenized_word)\n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "bad_words_ids = get_tokens_as_list(word_list=[\"#\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - use generate since pipe is wrapper around generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def make_chat1(input_text: str, sys_prompt: str=\"\"):\n",
    "    \n",
    "    return f\"\"\"### System:\n",
    "{sys_prompt}\n",
    "\n",
    "### Uživatel:\n",
    "{input_text}\n",
    "\n",
    "### Asistent:\n",
    "\"\"\"\n",
    "\n",
    "def make_chat2(ins: str, context: Union[str | None] = None):\n",
    "    \"\"\"\n",
    "    Prepare the input text for the model during inference\n",
    "    \"\"\"\n",
    "    instruction = ins\n",
    "    context = context  # Handle missing context\n",
    "\n",
    "    prompt = \"Odpovězte na danou instrukci na základě poskytnutého kontextu (pokud nějaký je).\\n\\n\"\n",
    "    prompt += f\"### Instrukce: \\n{instruction}\\n\"\n",
    "\n",
    "    if context:\n",
    "        prompt += f\"### Kontext:\\n{context}\\n\"\n",
    "\n",
    "    prompt += \"### Odpoved:\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpovězte na danou instrukci na základě poskytnutého kontextu (pokud nějaký je).\n",
      "\n",
      "### Instrukce: \n",
      "Jaké je hlavní město Austrálie?\n",
      "### Odpoved:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = make_chat1(\n",
    "    \"Jak by někdo popsal sentiment tohoto tweetu?\\n@hummingbird604 yea...stává se hodně b/c mé bývalé exhibicionistické kariéry...mám volné vstupenky do většiny filmů...končí s 2 vzít peeps\",\n",
    "    \"Jsi asistentka AI. Budete mít úkol. Musíte vytvořit podrobnou a dlouhou odpověď.\"\n",
    ")\n",
    "example = make_chat2(\n",
    "    \"Jaké je hlavní město Austrálie?\"\n",
    ")\n",
    "example_encoded = new_tokenizer.encode(example, return_tensors=\"pt\").to(torch.device(\"cuda\"))\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpovězte na danou instrukci na základě poskytnutého kontextu (pokud nějaký je).\n",
      "\n",
      "### Instrukce: \n",
      "Jaké je hlavní město Austrálie?\n",
      "### Odpoved:\n",
      "Melbourne. #comment=1;&q... ----\n",
      "----)-- -->... ----- --! ======= \"Na to se ptá každý, ale nikdy jsme nic nevymysleli.\"\n",
      "\"Neříkal jsem vám někdy už tohle slovo?\" zeptal se jeden z mužů v uniformě urostlého důstojníka s plnovousem a za ním následovala řada důstojníků po dvou až třech vedle sebe ve dvojstupu seřazených podle hodností nad sebou od nejvyšších hodnostních stupínků pod nimi k nejnižšímu stupni první hodnosti před plukovníkem přes nižší důstojnictvo kolem nižších šarží ke kapitánovi do poslední skupiny kapit\n"
     ]
    }
   ],
   "source": [
    "new_model = new_model.to(torch.device(\"cuda\"))\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = new_model.generate(\n",
    "        input_ids=example_encoded,\n",
    "        max_new_tokens=128,\n",
    "#         num_beams=5,\n",
    "        do_sample=True,\n",
    "        top_k=15,\n",
    "        top_p=0.99,\n",
    "#         temperature=0.1,\n",
    "        repetition_penalty=1.5,\n",
    "#         penalty_alpha=0.6,\n",
    "        num_return_sequences=1,\n",
    "#       eos_token_id=terminators,\n",
    "        eos_token_id=new_model.config.eos_token_id,\n",
    "    )\n",
    "op = new_tokenizer.decode(generation_output[0], skip_special_tokens=False)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prvé trénovanie:\n",
    " -   alpha: 16, r: 8, batch_size: 4\n",
    " - výsledky: nanič\n",
    " - pri trénovaní sa loss postupne znižovala z 3.1 na 1.7, nasledoval spike na cca 2.8 potom loss postupne klesal \n",
    " - a toto sa opakovalo do konca (250 krokov, cca 4 takéto spiky)\n",
    " \n",
    "Ďalšie pokusy:\n",
    " - len ABSTRACT2HEADLINE\n",
    " - alpha: 16, r: 64, batch_size:8, group_by_length = False\n",
    " - opäť neúspech, menej oscilácie v loss\n",
    " \n",
    " - možno zaujímavé pozireť aj: https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/peft-flan-t5-int8-summarization.ipynb\n",
    "   - trénujú to cez Sequence2SequenceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='csmpt7b-ft-SumeCzech', vocab_size=64000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '[EOS]', 'unk_token': '[UNK]', 'pad_token': '[EOS]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64000: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64001: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.decode(torch.tensor([2]), skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "046b7d5b13104068b60e0b67f7c8313d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16a9c451d8424877ba84bd021334e9ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "234bc12a982c41ae93b93156856c3a81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ea80c14f1314b5c95fa997b67420dc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45c95b08e74845af90fe05f1ae826e14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bd047b485aa4e6d86cd4cda101cc98f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5ba881790f914e77ab171d906abc9d8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d993c8164da4d42b1d1a00a171dd483": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45c95b08e74845af90fe05f1ae826e14",
      "placeholder": "​",
      "style": "IPY_MODEL_234bc12a982c41ae93b93156856c3a81",
      "value": " 10000/10000 [00:20&lt;00:00, 560.20 examples/s]"
     }
    },
    "67f7b7125a4e42aabdd3614b3f8a079d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6be2100c4c7f4605a4ed20d59627a830": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_754e9f27204242f09fa7a3f59373512e",
       "IPY_MODEL_71bf17a0908d48b4ad8e444ea014398f",
       "IPY_MODEL_5d993c8164da4d42b1d1a00a171dd483"
      ],
      "layout": "IPY_MODEL_fc0a449394a4422abb814f4f1e8d9319"
     }
    },
    "71bf17a0908d48b4ad8e444ea014398f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cfc37e675eb420590003bdd30eef482",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16a9c451d8424877ba84bd021334e9ee",
      "value": 10000
     }
    },
    "7486ae8153d444a6b57df2ca840490ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f8abfefc24942f4ad5fd62262b06168",
      "placeholder": "​",
      "style": "IPY_MODEL_2ea80c14f1314b5c95fa997b67420dc9",
      "value": " 3/3 [01:08&lt;00:00, 22.76s/it]"
     }
    },
    "754e9f27204242f09fa7a3f59373512e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9745305d1cf6400e985450fcee4cca12",
      "placeholder": "​",
      "style": "IPY_MODEL_d3bb4824364e485aa81859e4762df7a2",
      "value": "Map: 100%"
     }
    },
    "7cfc37e675eb420590003bdd30eef482": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f26b29ee13c4f6e95e8cd723410718f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67f7b7125a4e42aabdd3614b3f8a079d",
      "placeholder": "​",
      "style": "IPY_MODEL_046b7d5b13104068b60e0b67f7c8313d",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "8f8abfefc24942f4ad5fd62262b06168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9745305d1cf6400e985450fcee4cca12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb2a038fcda44e4da4d89e3767fa8911": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0f505d1cc81485989225fb328bc396a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ba881790f914e77ab171d906abc9d8d",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4bd047b485aa4e6d86cd4cda101cc98f",
      "value": 3
     }
    },
    "d3bb4824364e485aa81859e4762df7a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3dd5ef4623e406daa5d2f019570f3b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f26b29ee13c4f6e95e8cd723410718f",
       "IPY_MODEL_d0f505d1cc81485989225fb328bc396a",
       "IPY_MODEL_7486ae8153d444a6b57df2ca840490ee"
      ],
      "layout": "IPY_MODEL_cb2a038fcda44e4da4d89e3767fa8911"
     }
    },
    "fc0a449394a4422abb814f4f1e8d9319": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
