{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb33cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "from accelerate import Accelerator\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbeb6227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44454\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"json\",name=\"SumeCzech\", data_files=\"sumeczech/sumeczech-1.0-test.jsonl\", split=\"train\", num_proc=64)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8542427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /storage/praha1/home/jurajdedic/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"BUT-FIT/csmpt7b\"\n",
    "new_model_path = \"csmpt7b-ft-SumeCzech-v0.2-r64-64\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\") # HF token TODO: zahodit do pice lebo public repo xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bf40a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/praha1/home/jurajdedic/.cache/huggingface/modules/transformers_modules/BUT-FIT/csmpt7b/30a18e16fcd9fc35c26e02d42cf058e131482df5/configuration_mpt.py:116: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n",
      "  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1eaf7b191b4d639343a4e15495e2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# base_model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf865ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MPTForCausalLM(\n",
       "      (transformer): MPTModel(\n",
       "        (wte): SharedEmbedding(64002, 4096)\n",
       "        (emb_drop): Dropout(p=0, inplace=False)\n",
       "        (blocks): ModuleList(\n",
       "          (0-31): 32 x MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (out_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=16384, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig.from_pretrained(new_model_path)\n",
    "new_model = get_peft_model(base_model, lora_config)\n",
    "new_model.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3acdc29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40d51de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, '[EOS]')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347b404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_base_model(example):\n",
    "    return f\"\"\"Tohle je článek: {example[\"text\"]}. A zde je stručný nadpis výše uvedeného článku v jedné větě: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6561175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_csmpt7b_ft(example):\n",
    "    return f\"\"\"### Instrukce:\n",
    "Použij zadaný vstup a napiš stručný výstup k splnění níže uvedeného úkolu:\n",
    "\n",
    "### Úkol:\n",
    "Sumarizuj vstupní abstrakt na nadpis.\n",
    "\n",
    "### Vstup:\n",
    "{example[\"text\"]}\n",
    "\n",
    "### Výstup:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e72e141d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instrukce:\\nPoužij zadaný vstup a napiš stručný výstup k splnění níže uvedeného úkolu:\\n\\n### Úkol:\\nSumarizuj vstupní abstrakt na nadpis.\\n\\n### Vstup:\\nPo očištění o sezónní a kalendářní vlivy rostl maloobchod meziročně o 4,2 procenta. Letošní srpen měl oproti tomu loňskému o jeden pracovní den více. Meziměsíčně očištěný růst dosáhl 1,1 procenta.Lidé utráceli výrazně více i v motoristickém segmentu, reálně o 7,0 procenta. Ten se do maloobchodu normálně nepočítá. Pokud se ale sečte růst v maloobchodě včetně motoristické segmentu, je meziroční růst 6,3 procenta.V nespecializovaných prodejnách rostly tržby o 3,9 procenta, přitom vyšší nárůst vykázaly prodejny s převahou nepotravinářského zboží než v prodejnách s převahou potravin. Ve specializovaných prodejnách se výrazně zvýšil prodej u prodejen potravin a to o 8,3 procenta. Lidé nejvíce utráceli za textil a obuv, kde tržby vzrostly o 15,9 procenta. Na odbyt šla i elektronika, železářské zboží, nábytek a další zboží pro domácnost. Podle ekonomů ale tento trend nebude dlouho pokračovat. Podle Pavla Sobíška z HVB Bank vysoký růst maloobchodu způsobil větší počet pracovních dní a chladnější počasí, které hnalo turisty spíše do obchodů než za památkami.\"Ani jeden ze dvou hlavních tahounů srpnových tržeb, tedy vyšší počet pracovních dní a vyšší prodej nových aut, není přenosný do dalšího měsíce,\" doplnil Sobíšek.Ani David Navrátil z České spořitelny si nemyslí, že růst tento trend udrží. \"Růst cen benzínu (minulý měsíc), plynu (říjen a leden), elektřiny, TV poplatků a spotřební daně z cigaret (leden), na to naváže MHD... to vše zatíží rodinné rozpočty a na další nákupy zbude méně peněz,\" uvedl Navrátil.Také podle Anne-Francoise Blüherové z Komerční banky se dá předpokládat pokles tempa růstu. \"V dalším měsíci očekáváme opět desceleraci díky slabším prodejům aut a nižšímu počtu pracovních dnů,\" řekla analytička KB.\\nPodle Eurostatu vzrostly meziročně kalendářně očištěné maloobchodní tržby v celé Evropské unii o 2,2 procenta. Nejlepší výsledky vykázaly Lotyšsko (o 22,6 procenta), Litva (o 14 procent) a Slovinsko (o 13,8 procenta). Naopak snížení tržeb v srpnu pozorovali v Lucembursku (o 4,4 procenta) a v Německu (o 0,8 procenta)\\n\\n### Výstup:'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_csmpt7b_ft(test_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09f24a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    3,     3,     3,  1380, 20222,    26,   199,  9650,   448,    74,\n",
      "         55868,  2170,   275, 34644, 36403,  5687,   276, 11309,  7046, 16110,\n",
      "         23291,    26,   199,   199,     3,     3,     3,  1447,  6802,    26,\n",
      "           199,    51,  1122,  1148, 31943, 10891, 22884,   696,   291, 38647,\n",
      "            14,   199,   199,     3,     3,     3, 11675,    26,   199,   828,\n",
      "         52859,   285, 43487,   275, 34854, 20774, 46760, 29315,  2492, 20139,\n",
      "           285,   738,    12,    18,  6009,    14, 26651, 26994,  1271,  5261,\n",
      "          1014, 63343,   285,  1445,  1822,   958,   774,    14,  4160, 10470,\n",
      "          1012,  3527, 18224,  5258, 14066,   390,    12,    17,  6009,    14,\n",
      "         17581, 15839, 39148,  3787,   774,   394,   263, 14255, 21045, 22738,\n",
      "            12, 21195,   285,   994,    12,    16,  6009,    14,  1703,   301,\n",
      "           305, 29315,  4143, 10494, 30696,    14,  1286,   301,   440,   301,\n",
      "          3809,  5258,   263, 29315, 34019,  1969, 14255,  2968, 22738,    12,\n",
      "           300, 35998,  5258,   914,    12,    19,  6009,    14,    54, 42623,\n",
      "           333, 10204,  2413, 30715, 33219, 22392,   285,   598,    12,    25,\n",
      "          6009,    12,  3282,  2259, 11175, 22575,  9899, 18113,   260, 62735,\n",
      "         34825,   442,  5580,   822,  2493,   701,   263, 30715,   260, 62735,\n",
      "          7342,    14,  1202, 29479, 30715,   301,  3787, 17347,  2349,   317,\n",
      "         28302,  7342,   275,   326,   285,  1060,    12,    19,  6009,    14,\n",
      "          8290,  2759, 15839, 39148,   320, 26588,   275, 16364,    12,   757,\n",
      "         22392, 37768,   285,  1305,    12,    25,  6009,    14,   652, 47685,\n",
      "          6542,   394, 41695,    12, 51970,  2344,  2493,    12, 10872,   275,\n",
      "           647,  2493,   306, 14074,    14,  2697, 60201,   440,  1128, 11051,\n",
      "          1930,  2184,  5168,    14,  2697,  9858,   366, 31394,  2283,   271,\n",
      "         63363,    34, 17269,  6981,  5258, 29315,  4143, 14829,  1761,  2388,\n",
      "          7214,  2817,   275, 48221,  4089,    12,   479,   312, 16907, 17580,\n",
      "          2801,   305, 12379,   701,   320, 56581,  1534, 14213,  1445,   597,\n",
      "          1577,  7932, 62063,  1400,  4674, 15273, 18720,    12,  1031,  2259,\n",
      "          2388,  7214,  2817,   275,  2259,  2349,  2991,  6960,    12,   764,\n",
      "         37229,   305,  4954,  4220,  1198, 15176,   366, 31394,  2416,    14,\n",
      "         14213,  6945, 45281,   271,  1996, 26043,   426, 31405,    12,   380,\n",
      "          5258,  1128, 11051, 16249,    14,   512, 28046,   267,  1425, 36345,\n",
      "           427,  8154,  1483,  4034,   860,  6242,   427,   318,  1291,   275,\n",
      "         26126,   860, 10963,    12,  6084, 10554,   275, 26863,  7447,   271,\n",
      "         29670,   427,   281,   583,   860,   291,   326, 52423, 15617,   531,\n",
      "           326,   591,  9415,   461, 11061, 52454,   275,   291,   647, 18136,\n",
      "         39847,  2601,  2445,  1198,  3734, 45281,    14, 10853,   854, 51028,\n",
      "            13, 13612,   724, 13901, 47801,  4328,  1416,  2192,   271, 31456,\n",
      "          6055,   301,   676, 17153,  8049, 43350,  7970,    14,   512,    54,\n",
      "          4352,  9031, 30265,  1818, 16965,   281,  2581,  1585,  7416,   918,\n",
      "          2349,   794,  6960,   275,  4194,   395,  4478,  7214,  4190,  1198,\n",
      "          5510,  6629, 31183, 33736,    14,   199,  4935,  6803,   720,    85,\n",
      "         37768, 20139, 10407,   304,  3527,  9432, 35132, 22392,   263,  1521,\n",
      "          5594, 19540,   285,   424,    12,    18,  6009,    14,  4080,  3454,\n",
      "         22575,  9899, 37283,  1013,   427,    79,  3125,    12,    22,  6009,\n",
      "           860, 11290,   352,   427,    79,  1802,  3459,     9,   275, 51805,\n",
      "           427,    79,  1978,    12,    24,  6009,   718,  8470,  5871, 18720,\n",
      "           263, 13127, 53517,   263, 35147,   814,   427,    79,   738,    12,\n",
      "            20,  6009,     9,   275,   263,  7630,   427,    79,  1533,    12,\n",
      "            24,  6009,     9,   199,   199,     3,     3,     3, 35103,    26]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "example_encoded = tokenizer.encode(template_csmpt7b_ft(test_dataset[2]), return_tensors=\"pt\", padding=True).to(torch.device(\"cuda\"))\n",
    "print(example_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc37b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "#     new_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    tokenizer.convert_tokens_to_ids(\".\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e192b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55728dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, '[EOS]')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68d896eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "V srpnu se meziročně zvýšily tržby za nepotravinářské zboží o 7,3 procenta, za potraviny o 1,2 procenta a za pohonné hmoty o 0,\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = new_model.generate(\n",
    "        input_ids=example_encoded,\n",
    "        max_new_tokens=32,\n",
    "        num_beams=2,\n",
    "        do_sample=True,\n",
    "#         top_k=10,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "    )\n",
    "\n",
    "generation_output_only = generation_output[0][example_encoded.size()[1]:]\n",
    "    \n",
    "op = tokenizer.decode(generation_output_only, skip_special_tokens=False)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "889a6a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16675,   300,  2998,    26,   506, 52859,   285, 43487,   275, 34854,\n",
      "        20774, 46760, 29315,  2492, 20139,   285,   738,    12,    18,  6009,\n",
      "           14, 26651, 26994,  1271,  5261,  1014, 63343,   285,  1445,  1822,\n",
      "          958,   774,    14,  4160, 10470,  1012,  3527, 18224,  5258, 14066,\n",
      "          390,    12,    17,  6009,    14, 17581, 15839, 39148,  3787,   774,\n",
      "          394,   263, 14255, 21045, 22738,    12, 21195,   285,   994,    12,\n",
      "           16,  6009,    14,  1703,   301,   305, 29315,  4143, 10494, 30696,\n",
      "           14,  1286,   301,   440,   301,  3809,  5258,   263, 29315, 34019,\n",
      "         1969, 14255,  2968, 22738,    12,   300, 35998,  5258,   914,    12,\n",
      "           19,  6009,    14,    54, 42623,   333, 10204,  2413, 30715, 33219,\n",
      "        22392,   285,   598,    12,    25,  6009,    12,  3282,  2259, 11175,\n",
      "        22575,  9899, 18113,   260, 62735, 34825,   442,  5580,   822,  2493,\n",
      "          701,   263, 30715,   260, 62735,  7342,    14,  1202, 29479, 30715,\n",
      "          301,  3787, 17347,  2349,   317, 28302,  7342,   275,   326,   285,\n",
      "         1060,    12,    19,  6009,    14,  8290,  2759, 15839, 39148,   320,\n",
      "        26588,   275, 16364,    12,   757, 22392, 37768,   285,  1305,    12,\n",
      "           25,  6009,    14,   652, 47685,  6542,   394, 41695,    12, 51970,\n",
      "         2344,  2493,    12, 10872,   275,   647,  2493,   306, 14074,    14,\n",
      "         2697, 60201,   440,  1128, 11051,  1930,  2184,  5168,    14,  2697,\n",
      "         9858,   366, 31394,  2283,   271, 63363,    34, 17269,  6981,  5258,\n",
      "        29315,  4143, 14829,  1761,  2388,  7214,  2817,   275, 48221,  4089,\n",
      "           12,   479,   312, 16907, 17580,  2801,   305, 12379,   701,   320,\n",
      "        56581,  1534, 14213,  1445,   597,  1577,  7932, 62063,  1400,  4674,\n",
      "        15273, 18720,    12,  1031,  2259,  2388,  7214,  2817,   275,  2259,\n",
      "         2349,  2991,  6960,    12,   764, 37229,   305,  4954,  4220,  1198,\n",
      "        15176,   366, 31394,  2416,    14, 14213,  6945, 45281,   271,  1996,\n",
      "        26043,   426, 31405,    12,   380,  5258,  1128, 11051, 16249,    14,\n",
      "          512, 28046,   267,  1425, 36345,   427,  8154,  1483,  4034,   860,\n",
      "         6242,   427,   318,  1291,   275, 26126,   860, 10963,    12,  6084,\n",
      "        10554,   275, 26863,  7447,   271, 29670,   427,   281,   583,   860,\n",
      "          291,   326, 52423, 15617,   531,   326,   591,  9415,   461, 11061,\n",
      "        52454,   275,   291,   647, 18136, 39847,  2601,  2445,  1198,  3734,\n",
      "        45281,    14, 10853,   854, 51028,    13, 13612,   724, 13901, 47801,\n",
      "         4328,  1416,  2192,   271, 31456,  6055,   301,   676, 17153,  8049,\n",
      "        43350,  7970,    14,   512,    54,  4352,  9031, 30265,  1818, 16965,\n",
      "          281,  2581,  1585,  7416,   918,  2349,   794,  6960,   275,  4194,\n",
      "          395,  4478,  7214,  4190,  1198,  5510,  6629, 31183, 33736,    14,\n",
      "          199,  4935,  6803,   720,    85, 37768, 20139, 10407,   304,  3527,\n",
      "         9432, 35132, 22392,   263,  1521,  5594, 19540,   285,   424,    12,\n",
      "           18,  6009,    14,  4080,  3454, 22575,  9899, 37283,  1013,   427,\n",
      "           79,  3125,    12,    22,  6009,   860, 11290,   352,   427,    79,\n",
      "         1802,  3459,     9,   275, 51805,   427,    79,  1978,    12,    24,\n",
      "         6009,   718,  8470,  5871, 18720,   263, 13127, 53517,   263, 35147,\n",
      "          814,   427,    79,   738,    12,    20,  6009,     9,   275,   263,\n",
      "         7630,   427,    79,  1533,    12,    24,  6009,   718,   419,   989,\n",
      "          300, 36403, 38647,  3155, 16110,  3819,   263,  2874, 31594,    26,\n",
      "          221, 44433,    53,  1002,   423, 16468,   263,  1970, 33219,   263,\n",
      "        13127, 17954,   320,  3309,   914,  3929, 44433,    14],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(generation_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d8ec2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/vestec1-elixir/home/jurajdedic\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d257b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "SUMECZECH_TEST = \"./sumeczech/sumeczech-1.0-test.jsonl\"\n",
    "\n",
    "class SummaryDatasetSumeCzech(Dataset):\n",
    "    def __init__(self, data_path: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (str): Path to the data file\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        with open(data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = SummaryDatasetSumeCzech(os.path.join(SUMECZECH_TEST))\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "198dd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_0shot_abstract_to_headline = {\n",
    "    \"dataset\": \"sumeczech\",\n",
    "    \"type\": \"0shot_abstract_to_headline\",\n",
    "    \"model\": \"llama3-8b-instruct\",\n",
    "    \"abstracts\" : [],\n",
    "    \"references\": [],\n",
    "    \"predictions\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f797430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:64001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, batch in enumerate(data_loader):\n",
    "    # get abstract and headline\n",
    "\n",
    "    abstract = batch['abstract'][0]\n",
    "    headline = batch['headline'][0]\n",
    "    text = batch['text'][0]\n",
    "\n",
    "#     format_content_abstract = \"'abstract': '{abstract}'\"\n",
    "#     format_content_text = \"'text': '{text}'\"\n",
    "#     format_content_abstract_0shot = \"{abstract}\"\n",
    "\n",
    "    # create prompt\n",
    "    # chat_abstract_to_headline_3shot[-1]['content'] = format_content_abstract.format(abstract=abstract)\n",
    "    # chat_text_to_abstract_3shot[-1]['content'] = format_content_text.format(text=text)\n",
    "    \n",
    "    \n",
    "    example_formatted = template_csmpt7b_ft({\"text\": abstract})\n",
    "\n",
    "    # prompt_3shot_abstract_to_headline = tokenizer.apply_chat_template(chat_abstract_to_headline_3shot, tokenize=False)\n",
    "    # prompt_3shot_text_to_abstract = tokenizer.apply_chat_template(chat_text_to_abstract_3shot, tokenize=False)\n",
    "#     prompt_0shot_abstract_to_headline = tokenizer.apply_chat_template(chat_abstract_to_headline_0shot, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "#     print(example_formatted)\n",
    "\n",
    "    example_encoded = tokenizer.encode(example_formatted, return_tensors=\"pt\", padding=True).to(torch.device(\"cuda\"))\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\".\")\n",
    "    ]\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        generation_output = new_model.generate(\n",
    "            input_ids=example_encoded,\n",
    "            max_new_tokens=32,\n",
    "            num_beams=3,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            length_penalty=1.5,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=terminators,\n",
    "        )\n",
    "\n",
    "    generation_output_only = generation_output[0][example_encoded.size()[1]:]\n",
    "\n",
    "    prediction = tokenizer.decode(generation_output_only, skip_special_tokens=False).strip()\n",
    "\n",
    "    # save json abstracts and their references + predictions\n",
    "    # json_data_3shot_abstract_to_headline[\"abstracts\"].append(abstract)\n",
    "    # json_data_3shot_abstract_to_headline[\"references\"].append(headline)\n",
    "    # json_data_3shot_abstract_to_headline[\"predictions\"].append(prediction)\n",
    "\n",
    "    # json_data_3shot_text_to_abstract[\"texts\"].append(text)\n",
    "    # json_data_3shot_text_to_abstract[\"references\"].append(abstract)\n",
    "    # json_data_3shot_text_to_abstract[\"predictions\"].append(prediction)\n",
    "\n",
    "    json_data_0shot_abstract_to_headline[\"abstracts\"].append(abstract)\n",
    "    json_data_0shot_abstract_to_headline[\"references\"].append(headline)\n",
    "    json_data_0shot_abstract_to_headline[\"predictions\"].append(prediction)\n",
    "\n",
    "    if i >= 150:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b01619e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_data_0shot_abstract_to_headline[\"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d51b12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'sumeczech',\n",
       " 'type': '0shot_abstract_to_headline',\n",
       " 'model': 'llama3-8b-instruct',\n",
       " 'abstracts': ['Kdo hledá do interiéru tak trochu jinou podlahu, určitě dříve či později objeví přírodní marmoleum, nové dekory měkčeného PVC nebo sametový vinyl s názvem Flotex, který nejvíce připomíná nakrátko střižený koberec. Jsou to všechno materiály nejen krásné, ale hlavně praktické a cenově přijatelné.',\n",
       "  'Policisté po sedmi měsících zadrželi muže, který v lednu v Orlové na Karvinsku zneužil chlapce a dívku, kterým ještě nebylo ani deset let. Dvaatřicetiletého muže obvinili ze znásilnění. Hrozí mu až 12 let vězení.',\n",
       "  'Tržby maloobchodu se v srpnu meziročně zvedly o 5,9 procenta. To představuje nejvyšší růst od listopadu roku 2004. Nejvíce se na něm podílel prodej textilu a obuvi. Uvedl to v úterý Český statistický úřad (ČSÚ).',\n",
       "  'Valtické Podzemí pozvalo Dvorní divadlo Hlohovec, aby v unikátním sklepním labyrintu pro začátek odehrálo jedno ze svých čtyř představení. Ve Valtickém Podzemí, kde mívají lidé myšlenku spíše na sklenku vína, se už konalo nepřeberně akcí, divadlo však dosud nikoliv.',\n",
       "  'Tak jako její věčné převleky působí i zpěvaččina třetí deska Artpop místy okoukaně. Není to revoluce, ale baví. Ke slibovanému odhalení a odhození přetvářky však nedošlo.',\n",
       "  'Byla to naprosto nestandardní a neočekávaná situace, na kterou není možné se nijak připravit. Tak o brutálním napadení mistra odborného výcviku žákem prvního ročníku mluví ředitel Střední školy polytechnické v Olomouci Aleš Jurečka. Škola podle něj nemá možnosti, jak podobným incidentům zabránit.'],\n",
       " 'references': ['Trendy podlahy vyzývají ke kreativitě i návratu k přírodě',\n",
       "  'Kriminalisté dopadli násilníka, který v lednu zneužil školáky z Orlové',\n",
       "  'Maloobchod v srpnu výrazně rostl',\n",
       "  'Do Valtického Podzemí za divadlem místo vína',\n",
       "  'Lady Gaga umí jen pop. Ale lehký, autorský a kvalitní',\n",
       "  'Brutální útok na učitele: Se žákem nebyly závažné problémy, říká ředitel'],\n",
       " 'predictions': ['Popiš výhody a nevýhody jednotlivých typů podlahových krytin a vyber tu, která je podle tebe nejvhodnější do tvého interiéru[EOS]',\n",
       "  'Policisté zadrželi muže, který v lednu v Orlové na Karvinsku zneužil chlapce a dívku, kterým ještě nebylo ani deset let[EOS]',\n",
       "  'Tržby maloobchodu se v srpnu meziročně zvedly o 5,9 procenta[EOS]',\n",
       "  '### Valtické Podzemí pozvalo Dvorní divadlo Hlohovec, aby v unikátním sklepním labyrintu pro začátek odehrálo jedno ze svých čtyř',\n",
       "  '###\\n\\n###\\n\\n###\\n\\n###\\n\\n###\\n\\n###',\n",
       "  'Ředitel školy Aleš Jurečka:\\n\"V současné době nemáme žádné možnosti, jak podobným incidentům zabránit.\"']}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data_0shot_abstract_to_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6785c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6588970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "df_json_data_0shot_abstract_to_headline = DataFrame.from_dict(json_data_0shot_abstract_to_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66d5cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_data_0shot_abstract_to_headline['predictions'] = df_json_data_0shot_abstract_to_headline['predictions'].str.replace('_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0531302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_data_0shot_abstract_to_headline.to_json(\"./sumeczech_0shot_abstract-to-headline.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
