{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynks1jlffxZe",
    "outputId": "10ce83f4-d401-461e-fcbc-a338fc23c48a"
   },
   "source": [
    "#### !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets \n",
    "# !pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "# !pip install peft\n",
    "# !pip install trl\n",
    "# !pip install transformers[torch] -U\n",
    "# !pip install accelerate -U\n",
    "# !pip install bitsandbytes\n",
    "\n",
    "# # be sure to install right flash-attn, we use torch compiled with CUDA 12.1, no ABI, python 3.9, Linux x86_64 architecture\n",
    "# !pip install flash-attn -U --no-build-isolation\n",
    " \n",
    "# !pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.3/flash_attn-2.5.3+cu122torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl --no-build-isolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this to clean GPU memory\n",
    "import torch\n",
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EVTNvJG7fxZf"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "from accelerate import Accelerator\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset & configure templates\n",
    "\n",
    "- *Can be skipped if dataset already on disk* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_gux7ajYfxZf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44454\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\",name=\"SumeCzech\", data_files=\"./sumeczech/sumeczech-1.0-test.jsonl\", split=\"train\", num_proc=64)\n",
    "# dataset = load_dataset(\"json\",name=\"TranslatedInstruct\", data_files=\"./datasets/translated_dataset.jsonl\", split=\"train\")\n",
    "dataset = dataset.shuffle(seed=69)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EctDCMg0fxZf"
   },
   "outputs": [],
   "source": [
    "# use only first 1000 examples\n",
    "dataset = dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z5Vj3tqXfxZg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def formatting_prompts_func(example, in_type, out_type):\n",
    "    \"\"\"\n",
    "    Prepare the input text for the model\n",
    "    in_type: [\"abstract\", \"text\"] what to summarize from\n",
    "    out_type: [\"abstract\",\"headline\"] what to summarize to\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        in_text = example[in_type]\n",
    "        out_text = example[out_type]\n",
    "    except:\n",
    "        print(\"ERROR\")\n",
    "        print(example)\n",
    "\n",
    "    def param_type_to_czech(param_type):\n",
    "        if param_type == \"abstract\":\n",
    "            return \"abstrakt\"\n",
    "        elif param_type == \"headline\":\n",
    "            return \"nadpis\"\n",
    "        else:\n",
    "            return param_type\n",
    "\n",
    "    task_instructions = {\n",
    "        (\"text\", \"abstract\"): [\n",
    "            \"Sumarizuj vstupní {in_type} na stručný {out_type}.\",\n",
    "#             \"Vytvoř výstižný {out_type} na základě zadaného {in_type}.\",\n",
    "            \"Přeformuluj zadaný {in_type} na stručný {out_type}.\",\n",
    "            \"Zpracuj vstupní {in_type} a napiš výstižný {out_type}.\",\n",
    "#             \"Shrň podstatné informace ze vstupního {in_type} do {out_type}.\"\n",
    "        ],\n",
    "        (\"text\", \"headline\"): [\n",
    "            \"Sumarizuj vstupní {in_type} na výstižný {out_type}.\",\n",
    "#             \"Vytvoř poutavý {out_type} na základě zadaného {in_type}.\",\n",
    "            \"Přeformuluj zadaný {in_type} na stručný {out_type}.\",\n",
    "            \"Zpracuj vstupní {in_type} a napiš výstižný {out_type}.\",\n",
    "#             \"Shrň podstatné informace ze vstupního {in_type} do poutavého {out_type}.\"\n",
    "        ],\n",
    "        (\"abstract\", \"headline\"): [\n",
    "            \"Sumarizuj vstupní {in_type} na výstižný {out_type}.\",\n",
    "#             \"Vytvoř poutavý {out_type} na základě zadaného {in_type}.\",\n",
    "            \"Přeformuluj zadaný {in_type} na stručný {out_type}.\",\n",
    "            \"Zpracuj vstupní {in_type} a napiš výstižný {out_type}.\",\n",
    "#             \"Shrň podstatné informace ze vstupního {in_type} do poutavého {out_type}.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    task_instruction = random.choice(task_instructions[(in_type, out_type)]).format(\n",
    "        in_type=param_type_to_czech(in_type),\n",
    "        out_type=param_type_to_czech(out_type))\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": task_instruction},\n",
    "            {\"role\": \"user\", \"content\": in_text},\n",
    "            {\"role\": \"assistant\", \"content\": out_text}\n",
    "        ]\n",
    "    } \n",
    "\n",
    "#     text = f\"\"\"### Instrukce:\n",
    "# {task_instruction}\n",
    "\n",
    "# ### Vstup:\n",
    "# {in_text}\n",
    "\n",
    "# ### Výstup:\n",
    "# {out_text}\n",
    "# <|endoftext|>[EOS]\"\"\"\n",
    "\n",
    "#     text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "# { task_instruction }<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "# { in_text }<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "# {{ model_answer_1 }}<|eot_id|><|start_header_id|>user<|end_header_id|>\"\"\"\n",
    "\n",
    "#     return {\"text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for sumeczech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_to_abstract = copy.deepcopy(dataset)\n",
    "dataset_abstract_to_headline = copy.deepcopy(dataset)\n",
    "dataset_text_to_headline = copy.deepcopy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "krbEHZZ5qCKx"
   },
   "outputs": [],
   "source": [
    "# transform dataset so it has only field \"text\" with formatted prompts\n",
    "dataset_text_to_abstract = dataset_text_to_abstract.map(\n",
    "    formatting_prompts_func,\n",
    "    remove_columns=dataset_text_to_abstract.column_names,\n",
    "    num_proc=64,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"in_type\":\"text\", \"out_type\": \"abstract\"}\n",
    ")\n",
    "dataset_abstract_to_headline = dataset_abstract_to_headline.map(\n",
    "    formatting_prompts_func,\n",
    "    remove_columns=dataset_abstract_to_headline.column_names,\n",
    "    num_proc=64,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"in_type\":\"abstract\", \"out_type\": \"headline\"}\n",
    ")\n",
    "dataset_text_to_headline = dataset_text_to_headline.map(\n",
    "    formatting_prompts_func,\n",
    "    remove_columns=dataset_text_to_headline.column_names,\n",
    "    num_proc=64,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"in_type\":\"text\", \"out_type\": \"headline\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Zpracuj vstupní text a napiš výstižný abstrakt.', 'role': 'system'}, {'content': 'Alena Valterová přitom bývala svého času docela známá postava, hlavně když hned\\npo roce 1989 založila Politickou stranu žen a matek. V posledních letech\\nvšak na politiku rezignovala, žila spíše v izolaci a snažila se hlavně vydělávat\\nna sebe i své poslední nezaopatřené dítě. Celkem měla se dvěma manžely čtyři\\nděti a vychovávala je v podstatě sama.\\nJe tomu něco přes rok, co jsem s paní Valterovou dělala rozhovor pro\\nčasopis Marianne. Ten ale nikdy nevyšel. Proč? Požadavek redakce zněl: \"Přepiš\\nto tak, aby bylo vidět, že si svůj život voře sama.\" Hm. To se mi příčilo – na\\njednu stranu jsem si sice trošinku myslela, že si toho Alena Valterová nemusela\\nna svá bedra nakládat tolik, ale na druhé straně se podle mě jen dožadovala\\npodpory, na kterou měla coby matka čtyř dětí od společnosti nárok. Dělala to\\novšem donkichotsky, s ohnivým rozhořčením, protože se upínala k vizi, ve\\nkterou kromě ní dohromady nikdo nevěřil (jako že by stát reálně zbavil\\nsamoživitelky existenčních starostí, nebo že by se ženy masově zpolitizovaly a\\nsvé požadavky si vybojovaly), znovu a znovu se vrhala do politiky, přitom\\nmusela pracovat na dva a více úvazků a ještě se starat o děti, a tím vším se\\nnesmírně vyčerpávala.\\nPřiznávám, že ten poslední\\nrozhovor nebyl snadný, protože na jakoukoli otázku Alena Valterová reagovala lavinou\\nstížností na nejrůznější nespravedlnosti, nepravosti a křivdy, které ji\\nv životě potkaly, a často přitom zabíhala do takových detailů, až jsem\\nztrácela niť. Měla jsem před sebou stresovaného člověka na pokraji sil. Jenže\\ntak ona působila vždycky.\\nPřesto mi rozhovor pomohl\\nnajít v běhu jejího života logiku.\\nKousky tady z něj ještě ocituji, snad pomohou lépe\\npochopit tuto zajímavou, rozpornou osobnost s drásajícím osudem. Na muže měla smůlu:\\n\"Můj první manžel byl egoista a cholerik, druhý manžel dobromyslný flegmatik,\"\\nvysvětlovala. V roce 1989, na počátku druhého manželství, se ale cítila\\nv osobním životě šťastná. \"Tehdy jsem byla na mateřské s Janou a\\nspokojená novomanželka,\" vzpomínala. Proto jí založení politické strany\\nnepřipadalo jako těžká věc. Věřila, že je důležité, aby se ženy účastnily\\ntvorby nových zákonů. A předvídala, že v nastávajícím ekonomicky\\nnestabilním období na tom budou nejhůř samoživitelky.\\nJejí ženská strana byla ve\\nsvé době populární, ale do voleb jít nemohla. Nesešlo se totiž potřebných deset\\ntisíc podpisů. \"Ztrácely se nám podpisové archy, také to troskotalo na tom, že\\nženy si pletly podpis archu s přihláškou ke členství.\" Alena Valterová byla\\npřesvědčená, že někdo její politické plány záměrně křížil. \"Nechodily mi\\ndopisy, na telefon nebyl spoleh, někdy se z něj ozývaly podivné zvuky,\\nzáměrně bránili mé komunikaci se světem.\" Kdo \"oni\",\\nnevěděla. Mohlo na tom být něco pravdy? To už\\nse nedovíme.\\nV roce 1991 se rozhodla\\nvystěhovat z Prahy, do malé vesničky mezi Jeseníkem a Javorníkem, hlavně\\nkvůli zdraví dětí. Manžel měl mezitím v Praze vydělávat peníze, jenže se\\nmu stýskalo a přijel za rodinou. V kraji ale byla dvacetiprocentní\\nnezaměstnanost, a tak se brzo ocitli bez peněz. Pronajali si hotel a Alena\\nValterová dřela a dřela, aby ho udržela v provozu. \"Zaměstnanci odešli,\\ntak jsem dělala, co bylo třeba, od pokojské přes kotelníka po kuchařku.\" I v téhle\\nhoničce si našla čas na politickou práci. Kandidovala jako nezávislá za Levý\\nblok, kampaň ale nedokončila kvůli nedostatku peněz. Navíc v té době, ve\\nsvých čtyřiceti letech, čekala čtvrté dítě. \"Honzík se narodil nedonošený, bez\\nkrevních destiček, řekli mi, že umře.\" Nakonec se po zásahu lékařů destičky\\nv krvi objevily, ale dítě stále vyžadovalo mimořádnou péči. Manžel vůbec\\nneuměl hospodařit s penězi, existenční potíže sílily a schylovalo se\\nk rozvodu. Když bylo Honzíkovi osm měsíců, Alena Valterová vydala knihu\\nPřipomínky od sporáku, v níž shrnula své politické názory a zkušenosti. Právě\\nv té době však Politická strana žen a matek zanikla.\\nRok a půl po narození\\nHonzíka se Alena Valterová zhroutila. \"V ten den jsem prostě usnula a když\\njsem se vzbudila, byl jiný den, než měl být. Slyšela jsem občas nějaké zvuky a\\nhlasy dětí, ale jen nezřetelně. Organismus prostě vypnul.\" V roce 1996, čerstvě\\npodruhé rozvedená a stále uprostřed existenčních starostí, se zúčastnila\\nprvních senátních voleb jako nezávislá za Republikánskou unii. \"Není to Sládkova\\nstrana, je pravicová a liberální, nevím, jestli ještě existuje,\" vysvětlovala. Ani\\ntuto kampaň nedokončila, musela se starat o umírající matku, později o\\nosamělého nemocného otce.\\n\"Uvažte, kolik jsem\\ntomuhle státu ušetřila peněz,” bilancovala svůj život, \"péčí o matku několik\\ndrahých lůžkotýdnů na onkologii, pak jsem se doma starala o otce. Cena péče o\\njedno dítě, než vyroste, se odhaduje na 500 000 Kč. Kde jsou ty peníze?\" ptala\\nse řečnicky a ihned vystihla systémovou příčinu. \"Práci žen v rodině,\\ndomácnosti a jejich pečovatelské aktivity nikdo neocení a ženy samy si pak sebe\\na svojí práce neváží a nejsou sebevědomé, protože nejsou úspěšné tržně.\"\\nA jak v závěru rozhovoru\\nviděla svou budoucnost? \"Až si odpracuji ten\\nluxus, že jsem si dovolila mít děti, budu pracovat, abych měla na vzdělání a\\ncestování. Taky záleží na tom, jestli mi důchod bude stačit alespoň na pronájem\\npopelnice.\"\\nTehdy se mi zdálo, že je na samém konci obtížné překážkové dráhy a bude si\\nsnad konečně moci vydechnout a žít trochu pohodově. Nedopadlo to tak.\\nČlověk se samozřejmě neubrání myšlence, že podobně předčasně zemřely i další\\ndvě ženy hlásící se k feminismu - Carola Biedermannová (autorka svého času\\nvelmi populárního feministického spisku Mstivá kantiléna) a Saša Berková.\\nNáhoda? Může to být i tím, že feminismu se jako určitého nástroje chápou ženy,\\nkteré od něj očekávají pomoc v drásající situaci hmotné nebo psychické –\\nkdyž se jí nedočkaly ze strany společnosti, eventuelně svých blízkých. Alena\\nValterová byla toho nejkrystaličtějším příkladem.', 'role': 'user'}, {'content': 'Je to už pár týdnů, co jsem se dozvěděla, že zemřela paní Alena Valterová. Od paní Zdenky Petákové, která se chystala napsat nekrolog, ale také už s odstupem mnoha měsíců. V druhé půlce roku 2008, kdy dobrovolně odešla ze života ve věku 55 let, totiž nikdo tuhle smutnou zprávu nešířil.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_text_to_abstract[69][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVDEjeKTqFfg",
    "outputId": "ca33de5b-88c0-45a1-8e44-4adf54e766b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Zpracuj vstupní abstrakt a napiš výstižný nadpis.', 'role': 'system'}, {'content': 'Je to už pár týdnů, co jsem se dozvěděla, že zemřela paní Alena Valterová. Od paní Zdenky Petákové, která se chystala napsat nekrolog, ale také už s odstupem mnoha měsíců. V druhé půlce roku 2008, kdy dobrovolně odešla ze života ve věku 55 let, totiž nikdo tuhle smutnou zprávu nešířil.', 'role': 'user'}, {'content': 'Dožívají se feministky v průměru nižšího věku?', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_abstract_to_headline[69][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Zpracuj vstupní text a napiš výstižný nadpis.', 'role': 'system'}, {'content': 'Alena Valterová přitom bývala svého času docela známá postava, hlavně když hned\\npo roce 1989 založila Politickou stranu žen a matek. V posledních letech\\nvšak na politiku rezignovala, žila spíše v izolaci a snažila se hlavně vydělávat\\nna sebe i své poslední nezaopatřené dítě. Celkem měla se dvěma manžely čtyři\\nděti a vychovávala je v podstatě sama.\\nJe tomu něco přes rok, co jsem s paní Valterovou dělala rozhovor pro\\nčasopis Marianne. Ten ale nikdy nevyšel. Proč? Požadavek redakce zněl: \"Přepiš\\nto tak, aby bylo vidět, že si svůj život voře sama.\" Hm. To se mi příčilo – na\\njednu stranu jsem si sice trošinku myslela, že si toho Alena Valterová nemusela\\nna svá bedra nakládat tolik, ale na druhé straně se podle mě jen dožadovala\\npodpory, na kterou měla coby matka čtyř dětí od společnosti nárok. Dělala to\\novšem donkichotsky, s ohnivým rozhořčením, protože se upínala k vizi, ve\\nkterou kromě ní dohromady nikdo nevěřil (jako že by stát reálně zbavil\\nsamoživitelky existenčních starostí, nebo že by se ženy masově zpolitizovaly a\\nsvé požadavky si vybojovaly), znovu a znovu se vrhala do politiky, přitom\\nmusela pracovat na dva a více úvazků a ještě se starat o děti, a tím vším se\\nnesmírně vyčerpávala.\\nPřiznávám, že ten poslední\\nrozhovor nebyl snadný, protože na jakoukoli otázku Alena Valterová reagovala lavinou\\nstížností na nejrůznější nespravedlnosti, nepravosti a křivdy, které ji\\nv životě potkaly, a často přitom zabíhala do takových detailů, až jsem\\nztrácela niť. Měla jsem před sebou stresovaného člověka na pokraji sil. Jenže\\ntak ona působila vždycky.\\nPřesto mi rozhovor pomohl\\nnajít v běhu jejího života logiku.\\nKousky tady z něj ještě ocituji, snad pomohou lépe\\npochopit tuto zajímavou, rozpornou osobnost s drásajícím osudem. Na muže měla smůlu:\\n\"Můj první manžel byl egoista a cholerik, druhý manžel dobromyslný flegmatik,\"\\nvysvětlovala. V roce 1989, na počátku druhého manželství, se ale cítila\\nv osobním životě šťastná. \"Tehdy jsem byla na mateřské s Janou a\\nspokojená novomanželka,\" vzpomínala. Proto jí založení politické strany\\nnepřipadalo jako těžká věc. Věřila, že je důležité, aby se ženy účastnily\\ntvorby nových zákonů. A předvídala, že v nastávajícím ekonomicky\\nnestabilním období na tom budou nejhůř samoživitelky.\\nJejí ženská strana byla ve\\nsvé době populární, ale do voleb jít nemohla. Nesešlo se totiž potřebných deset\\ntisíc podpisů. \"Ztrácely se nám podpisové archy, také to troskotalo na tom, že\\nženy si pletly podpis archu s přihláškou ke členství.\" Alena Valterová byla\\npřesvědčená, že někdo její politické plány záměrně křížil. \"Nechodily mi\\ndopisy, na telefon nebyl spoleh, někdy se z něj ozývaly podivné zvuky,\\nzáměrně bránili mé komunikaci se světem.\" Kdo \"oni\",\\nnevěděla. Mohlo na tom být něco pravdy? To už\\nse nedovíme.\\nV roce 1991 se rozhodla\\nvystěhovat z Prahy, do malé vesničky mezi Jeseníkem a Javorníkem, hlavně\\nkvůli zdraví dětí. Manžel měl mezitím v Praze vydělávat peníze, jenže se\\nmu stýskalo a přijel za rodinou. V kraji ale byla dvacetiprocentní\\nnezaměstnanost, a tak se brzo ocitli bez peněz. Pronajali si hotel a Alena\\nValterová dřela a dřela, aby ho udržela v provozu. \"Zaměstnanci odešli,\\ntak jsem dělala, co bylo třeba, od pokojské přes kotelníka po kuchařku.\" I v téhle\\nhoničce si našla čas na politickou práci. Kandidovala jako nezávislá za Levý\\nblok, kampaň ale nedokončila kvůli nedostatku peněz. Navíc v té době, ve\\nsvých čtyřiceti letech, čekala čtvrté dítě. \"Honzík se narodil nedonošený, bez\\nkrevních destiček, řekli mi, že umře.\" Nakonec se po zásahu lékařů destičky\\nv krvi objevily, ale dítě stále vyžadovalo mimořádnou péči. Manžel vůbec\\nneuměl hospodařit s penězi, existenční potíže sílily a schylovalo se\\nk rozvodu. Když bylo Honzíkovi osm měsíců, Alena Valterová vydala knihu\\nPřipomínky od sporáku, v níž shrnula své politické názory a zkušenosti. Právě\\nv té době však Politická strana žen a matek zanikla.\\nRok a půl po narození\\nHonzíka se Alena Valterová zhroutila. \"V ten den jsem prostě usnula a když\\njsem se vzbudila, byl jiný den, než měl být. Slyšela jsem občas nějaké zvuky a\\nhlasy dětí, ale jen nezřetelně. Organismus prostě vypnul.\" V roce 1996, čerstvě\\npodruhé rozvedená a stále uprostřed existenčních starostí, se zúčastnila\\nprvních senátních voleb jako nezávislá za Republikánskou unii. \"Není to Sládkova\\nstrana, je pravicová a liberální, nevím, jestli ještě existuje,\" vysvětlovala. Ani\\ntuto kampaň nedokončila, musela se starat o umírající matku, později o\\nosamělého nemocného otce.\\n\"Uvažte, kolik jsem\\ntomuhle státu ušetřila peněz,” bilancovala svůj život, \"péčí o matku několik\\ndrahých lůžkotýdnů na onkologii, pak jsem se doma starala o otce. Cena péče o\\njedno dítě, než vyroste, se odhaduje na 500 000 Kč. Kde jsou ty peníze?\" ptala\\nse řečnicky a ihned vystihla systémovou příčinu. \"Práci žen v rodině,\\ndomácnosti a jejich pečovatelské aktivity nikdo neocení a ženy samy si pak sebe\\na svojí práce neváží a nejsou sebevědomé, protože nejsou úspěšné tržně.\"\\nA jak v závěru rozhovoru\\nviděla svou budoucnost? \"Až si odpracuji ten\\nluxus, že jsem si dovolila mít děti, budu pracovat, abych měla na vzdělání a\\ncestování. Taky záleží na tom, jestli mi důchod bude stačit alespoň na pronájem\\npopelnice.\"\\nTehdy se mi zdálo, že je na samém konci obtížné překážkové dráhy a bude si\\nsnad konečně moci vydechnout a žít trochu pohodově. Nedopadlo to tak.\\nČlověk se samozřejmě neubrání myšlence, že podobně předčasně zemřely i další\\ndvě ženy hlásící se k feminismu - Carola Biedermannová (autorka svého času\\nvelmi populárního feministického spisku Mstivá kantiléna) a Saša Berková.\\nNáhoda? Může to být i tím, že feminismu se jako určitého nástroje chápou ženy,\\nkteré od něj očekávají pomoc v drásající situaci hmotné nebo psychické –\\nkdyž se jí nedočkaly ze strany společnosti, eventuelně svých blízkých. Alena\\nValterová byla toho nejkrystaličtějším příkladem.', 'role': 'user'}, {'content': 'Dožívají se feministky v průměru nižšího věku?', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_text_to_headline[69][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "dataset_new = concatenate_datasets([dataset_text_to_abstract, dataset_abstract_to_headline, dataset_text_to_headline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0effcc3184424b5cb6fc16d131a560d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/64 shards):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0effcc3184424b5cb6fc16d131a560d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/64 shards):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_new.save_to_disk(\"datasets/sumeczech-test-Llama-3t-prompt-format\", num_proc=64)\n",
    "print(dataset_new)\n",
    "# TODO: filter to max 2k sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & configure training \n",
    "- load model into the GPU\n",
    "- confogire LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pm9gHGlefxZg",
    "outputId": "764b8537-5e09-46f1-d8d3-8118af1219c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /storage/praha1/home/jurajdedic/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model_path = \"Llama-3-8b-ft-Sumeczech\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\") # HF token TODO: zahodit do pice lebo public repo xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198,
     "referenced_widgets": [
      "d3dd5ef4623e406daa5d2f019570f3b2",
      "8f26b29ee13c4f6e95e8cd723410718f",
      "d0f505d1cc81485989225fb328bc396a",
      "7486ae8153d444a6b57df2ca840490ee",
      "cb2a038fcda44e4da4d89e3767fa8911",
      "67f7b7125a4e42aabdd3614b3f8a079d",
      "046b7d5b13104068b60e0b67f7c8313d",
      "5ba881790f914e77ab171d906abc9d8d",
      "4bd047b485aa4e6d86cd4cda101cc98f",
      "8f8abfefc24942f4ad5fd62262b06168",
      "2ea80c14f1314b5c95fa997b67420dc9"
     ]
    },
    "id": "OiObI35wfxZg",
    "outputId": "21a673c1-c128-4dbb-ab07-4a7e35d85d30"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb6a51d7fcb4fd5930431b701613b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    base_model_name, \n",
    "    trust_remote_code=True,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "\n",
    "model.config.pretraining_tp = 1\n",
    "# model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the dataset using tokenizer\n",
    "\n",
    "- *Can be skipped if dataset already on disk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_length(example):\n",
    "    messages = example['messages']\n",
    "    \n",
    "    text_ids = tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    encoded_length = text_ids.shape[1]\n",
    "    \n",
    "    if encoded_length < (2048):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset samples before filtering: 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fee0e5076d41469a83fcba5cc4e8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=64):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset samples after filtering: 2881\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset samples before filtering:\", len(dataset_new))\n",
    "dataset_new = dataset_new.filter(is_good_length, num_proc=64)\n",
    "print(\"Dataset samples after filtering:\", len(dataset_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 129104\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9a57a5f34b4336820ee94ea6b291e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/64 shards):   0%|          | 0/2881 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_new.save_to_disk(\"datasets/sumeczech-test-Llama-3t-prompt-format\", num_proc=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If processed dataset on disk use this to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6eb6b3f3a74994a4f06f744d2ad88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09abbabb1d01432bb2466f4cad2cc715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"datasets/sumeczech-full-filter-Llama-3t-prompt-format\")\n",
    "\n",
    "dataset_eval = load_from_disk(\"datasets/sumeczech-test-Llama-3t-prompt-format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 129104\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 2881\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'gate_proj',\n",
       " 'up_proj',\n",
       " 'k_proj',\n",
       " 'v_proj',\n",
       " 'o_proj',\n",
       " 'q_proj',\n",
       " 'down_proj']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Conv1D\n",
    "\n",
    "def get_specific_layer_names(model):\n",
    "    # Create a list to store the layer names\n",
    "    layer_names = []\n",
    "    \n",
    "    # Recursively visit all modules and submodules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is an instance of the specified layers\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
    "            # model name parsing \n",
    "\n",
    "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
    "    \n",
    "    return layer_names\n",
    "\n",
    "list(set(get_specific_layer_names(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NHAZ2UW1fxZg"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32, # TODO: Mozno zmenit\n",
    "    lora_dropout=0.02,\n",
    "    r=16, # TODO: Mozno zmenit\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"up_proj\", \"down_proj\", \"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\", 'gate_proj'] #TODO: wte maybe\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161,
     "referenced_widgets": [
      "6be2100c4c7f4605a4ed20d59627a830",
      "754e9f27204242f09fa7a3f59373512e",
      "71bf17a0908d48b4ad8e444ea014398f",
      "5d993c8164da4d42b1d1a00a171dd483",
      "fc0a449394a4422abb814f4f1e8d9319",
      "9745305d1cf6400e985450fcee4cca12",
      "d3bb4824364e485aa81859e4762df7a2",
      "7cfc37e675eb420590003bdd30eef482",
      "16a9c451d8424877ba84bd021334e9ee",
      "45c95b08e74845af90fe05f1ae826e14",
      "234bc12a982c41ae93b93156856c3a81"
     ]
    },
    "id": "umJcFMBPfxZh",
    "outputId": "2e18393e-a5f5-4591-eb4d-ed6c3ddc8ef5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e7ceea3bd24df9a6d3dc0262b6fba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129104 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2, # TODO: uvidime kolko bude stacit\n",
    "    per_device_train_batch_size=1, # TODO: mozno zmenit\n",
    "    gradient_accumulation_steps=1, # TODO: mozno zmenit\n",
    "    optim=\"paged_adamw_32bit\", # adamw_torch_fused\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    logging_steps=50,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.000025,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.06,\n",
    "    group_by_length=False, #could cause the oscillation in loss (https://github.com/artidoro/qlora/issues/228)\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset_eval,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= 2048, # maximum pre BUT Tiny LLama\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     neftune_noise_alpha=5, #should improve the performance but needs to be tested\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "71iKvh-zfxZh",
    "outputId": "bfc19a89-3f80-4591-fed3-bb22c7bb7410"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(trainer.model)\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVYR0Gv0fxZh",
    "outputId": "d4525002-e110-45ef-a81e-57ee3dcd251f"
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model_path)\n",
    "trainer.tokenizer.save_pretrained(new_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for inference\n",
    "\n",
    "- build prompt\n",
    "- load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285459a5a70c48918b93f03d1c7ec931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129356\n"
     ]
    }
   ],
   "source": [
    "# test_dataset = load_from_disk(\"datasets/sumeczech_test-40k-3t-prompt-format-filter_1.5kseq\")\n",
    "# print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /storage/praha1/home/jurajdedic/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\") # HF token TODO: zahodit do pice lebo public repo xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf8c4244f354410994e0bf555a63896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "base_model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig.from_pretrained(new_model_path)\n",
    "new_model = get_peft_model(base_model, lora_config)\n",
    "new_model.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer = AutoTokenizer.from_pretrained(new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VrYWMvktfxZi"
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# pipe = pipeline(task=\"text-generation\", model=new_model, tokenizer=new_tokenizer, torch_dtype=torch.bfloat16, device_map=\"auto\", max_length=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Přeformuluj zadaný text na stručný nadpis.', 'role': 'system'}, {'content': 'Vleče se to jako na magistrátu. I takový příměr by se v Praze mohl ujmout kvůli tomu, jak dlouho trvá vyjednávání o nové koalici. Ta původní se rozpadla na začátku listopadu. A ani nyní v lednu není jisté, jak bude nová vláda složená.Navenek se TOP 09, ODS, Trojkoalice, nezávislí exprimátora Tomáše Hudečka a piráti tváří, že jednání běží hladce dál. Vnitřní vztahy však již zasáhly spory o program.Přesto v pondělí předseda klubu TOP 09 Václav Novotný oznámil, že se posunuli o krok dál. \"Už jsme ve fázi, kdy můžeme vytvořit další skupinu, která se jmenuje koaliční mechanismy. Z toho vidíte, že směřujeme ke konkrétní konstrukci koalice,\" uvedl Novotný.Primátorem by se mohl stát exnáměstek Jiří Nouza či Václav Novotný. Rozložení sil v radě třeba lídr Trojkoalice Petr Štěpánek (SZ) nekomentoval, nicméně by v ní měla usednout TOP 09, Trojkoalice a piráti. ODS a nezávislí \"hudečkovci\" budou jejich menšinovou vládu podporovat. I proto bude podle Novotného obtížnější nastavit v koaliční smlouvě to, jak budou ODS a nezávislí zasahovat do výkonu rady. Pravděpodobně dostanou funkce ve vedení výborů.Do konce týdne chtějí vyjednavači vyřešit programové rozpory. \"Valná většina pracovních skupin došla k převážné shodě, leč jsou tam ještě záležitosti, které musíme diskutovat třeba v dopravě či územním plánu,\" dodal Novotný. Problémy mohou ještě nastat u obsazování postů v představenstvech a dozorčích radách městských firem. V případě metropolitního plánu je klíčový nesoulad mezi Matějem Stropnickým (SZ) a exprimátorem Hudečkem, kteří musí hledat kompromis právě nad tímto dokumentem. Jeden považuje Metropolitní plán za dokončený a připravený k projednání, druhý ho chce přepracovat. V dopravě se liší pohledy na to, zda má město tlačit na dokončení městského okruhu v navrhované variantě a jakou podobu by měly mít nové části vnějšího okruhu. TOP 09 by je stavěla podle aktuálních plánů, Trojkoalice naopak prosazuje v některých případech změnu tras.Problémy se systémem obsazování představenstev a dozorčích rad městských či poloměstských firem měli hlavně piráti, kteří z nich původně chtěli \"vymýtit\" politiky.\"Nám jde hlavně o nastavení jasných pravidel obsazování těchto orgánů,\" řekl k tomu v pondělí jejich lídr Jakub Michálek. Do představenstev by usedali manažeři (politici jen výjimečně) a do dozorčích rad i politici (pokud možno s nějakou odbornou znalostí věci).Napínavý seriál o skládání koalice na magistrátu by se v následujících týdnech mohl odvíjet v zásadě dvojím způsobem. Zaprvé: Když TOP 09 \"a spol.\" uspěje, může mít Praha nového primátora v osobách exnáměstků buď Nouzy, nebo Novotného.Zadruhé: Není vyloučené, že spolupráce TOP 09, Trojkoalice, ODS a pirátů ztroskotá na programu, když se nedohodnou například na projektu Metropolitního plánu či na dopravě. Navíc sílí hlasy zastupitelů, kterým se do spojenectví moc nechce. Například ODS vadí angažmá Tomáše Hudečka a část klubu TOP 09 je rozpačitá z pirátů.\"Této koalici proto někteří zastupitelé říkají mediální - protože je líbivější navenek. Je možné, že to nakonec dopadne úplně jinak,\" řekl MF DNES jeden ze zdrojů. Přál si zůstat v anonymitě stejně jako jiní zastupitelé. Část z nich totiž preferuje koalici nazývanou \"praktická\", v níž by měla TOP 09 spolupracovat s hnutím ANO, ČSSD a případně ještě s dalším partnerem.Nebo se tandem ANO a ČSSD obrátí na Trojkoalici a urovnají spory. To je ostatně varianta, o níž někteří radní hovořili už pár minut po listopadovém rozpadu vlády.Původní rada ANO, ČSSD a Trojkoalice se rozpadla potom, co se strany rozhádaly a posléze si vzájemně odvolaly několik radních.', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "example = dataset_eval[1950][\"messages\"][:2]\n",
    "# example = example_raw.split(\"### Výstup:\")[0] + \"### Výstup:\"\n",
    "print(example)\n",
    "# print(example_raw.split(\"### Výstup:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    new_tokenizer.eos_token_id,\n",
    "#     pipe.tokenizer.convert_tokens_to_ids(\".\"),\n",
    "    new_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tokens_as_list(word_list):\n",
    "#     \"Converts a sequence of words into a list of tokens\"\n",
    "#     tokens_list = []\n",
    "#     for word in word_list:\n",
    "#         tokenized_word = new_tokenizer([word], add_special_tokens=False).input_ids[0]\n",
    "#         tokens_list.append(tokenized_word)\n",
    "#     return tokens_list\n",
    "\n",
    "\n",
    "# bad_words_ids = get_tokens_as_list(word_list=[\"#\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - use generate since pipe is wrapper around generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Přeformuluj zadaný text na stručný nadpis.', 'role': 'system'}, {'content': 'Vleče se to jako na magistrátu. I takový příměr by se v Praze mohl ujmout kvůli tomu, jak dlouho trvá vyjednávání o nové koalici. Ta původní se rozpadla na začátku listopadu. A ani nyní v lednu není jisté, jak bude nová vláda složená.Navenek se TOP 09, ODS, Trojkoalice, nezávislí exprimátora Tomáše Hudečka a piráti tváří, že jednání běží hladce dál. Vnitřní vztahy však již zasáhly spory o program.Přesto v pondělí předseda klubu TOP 09 Václav Novotný oznámil, že se posunuli o krok dál. \"Už jsme ve fázi, kdy můžeme vytvořit další skupinu, která se jmenuje koaliční mechanismy. Z toho vidíte, že směřujeme ke konkrétní konstrukci koalice,\" uvedl Novotný.Primátorem by se mohl stát exnáměstek Jiří Nouza či Václav Novotný. Rozložení sil v radě třeba lídr Trojkoalice Petr Štěpánek (SZ) nekomentoval, nicméně by v ní měla usednout TOP 09, Trojkoalice a piráti. ODS a nezávislí \"hudečkovci\" budou jejich menšinovou vládu podporovat. I proto bude podle Novotného obtížnější nastavit v koaliční smlouvě to, jak budou ODS a nezávislí zasahovat do výkonu rady. Pravděpodobně dostanou funkce ve vedení výborů.Do konce týdne chtějí vyjednavači vyřešit programové rozpory. \"Valná většina pracovních skupin došla k převážné shodě, leč jsou tam ještě záležitosti, které musíme diskutovat třeba v dopravě či územním plánu,\" dodal Novotný. Problémy mohou ještě nastat u obsazování postů v představenstvech a dozorčích radách městských firem. V případě metropolitního plánu je klíčový nesoulad mezi Matějem Stropnickým (SZ) a exprimátorem Hudečkem, kteří musí hledat kompromis právě nad tímto dokumentem. Jeden považuje Metropolitní plán za dokončený a připravený k projednání, druhý ho chce přepracovat. V dopravě se liší pohledy na to, zda má město tlačit na dokončení městského okruhu v navrhované variantě a jakou podobu by měly mít nové části vnějšího okruhu. TOP 09 by je stavěla podle aktuálních plánů, Trojkoalice naopak prosazuje v některých případech změnu tras.Problémy se systémem obsazování představenstev a dozorčích rad městských či poloměstských firem měli hlavně piráti, kteří z nich původně chtěli \"vymýtit\" politiky.\"Nám jde hlavně o nastavení jasných pravidel obsazování těchto orgánů,\" řekl k tomu v pondělí jejich lídr Jakub Michálek. Do představenstev by usedali manažeři (politici jen výjimečně) a do dozorčích rad i politici (pokud možno s nějakou odbornou znalostí věci).Napínavý seriál o skládání koalice na magistrátu by se v následujících týdnech mohl odvíjet v zásadě dvojím způsobem. Zaprvé: Když TOP 09 \"a spol.\" uspěje, může mít Praha nového primátora v osobách exnáměstků buď Nouzy, nebo Novotného.Zadruhé: Není vyloučené, že spolupráce TOP 09, Trojkoalice, ODS a pirátů ztroskotá na programu, když se nedohodnou například na projektu Metropolitního plánu či na dopravě. Navíc sílí hlasy zastupitelů, kterým se do spojenectví moc nechce. Například ODS vadí angažmá Tomáše Hudečka a část klubu TOP 09 je rozpačitá z pirátů.\"Této koalici proto někteří zastupitelé říkají mediální - protože je líbivější navenek. Je možné, že to nakonec dopadne úplně jinak,\" řekl MF DNES jeden ze zdrojů. Přál si zůstat v anonymitě stejně jako jiní zastupitelé. Část z nich totiž preferuje koalici nazývanou \"praktická\", v níž by měla TOP 09 spolupracovat s hnutím ANO, ČSSD a případně ještě s dalším partnerem.Nebo se tandem ANO a ČSSD obrátí na Trojkoalici a urovnají spory. To je ostatně varianta, o níž někteří radní hovořili už pár minut po listopadovém rozpadu vlády.Původní rada ANO, ČSSD a Trojkoalice se rozpadla potom, co se strany rozhádaly a posléze si vzájemně odvolaly několik radních.', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "example_encoded = new_tokenizer.apply_chat_template(example, tokenize=True, return_tensors=\"pt\", padding=True).to(torch.device(\"cuda\"))\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Přeformuluj zadaný text na stručný nadpis.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Vleče se to jako na magistrátu. I takový příměr by se v Praze mohl ujmout kvůli tomu, jak dlouho trvá vyjednávání o nové koalici. Ta původní se rozpadla na začátku listopadu. A ani nyní v lednu není jisté, jak bude nová vláda složená.Navenek se TOP 09, ODS, Trojkoalice, nezávislí exprimátora Tomáše Hudečka a piráti tváří, že jednání běží hladce dál. Vnitřní vztahy však již zasáhly spory o program.Přesto v pondělí předseda klubu TOP 09 Václav Novotný oznámil, že se posunuli o krok dál. \"Už jsme ve fázi, kdy můžeme vytvořit další skupinu, která se jmenuje koaliční mechanismy. Z toho vidíte, že směřujeme ke konkrétní konstrukci koalice,\" uvedl Novotný.Primátorem by se mohl stát exnáměstek Jiří Nouza či Václav Novotný. Rozložení sil v radě třeba lídr Trojkoalice Petr Štěpánek (SZ) nekomentoval, nicméně by v ní měla usednout TOP 09, Trojkoalice a piráti. ODS a nezávislí \"hudečkovci\" budou jejich menšinovou vládu podporovat. I proto bude podle Novotného obtížnější nastavit v koaliční smlouvě to, jak budou ODS a nezávislí zasahovat do výkonu rady. Pravděpodobně dostanou funkce ve vedení výborů.Do konce týdne chtějí vyjednavači vyřešit programové rozpory. \"Valná většina pracovních skupin došla k převážné shodě, leč jsou tam ještě záležitosti, které musíme diskutovat třeba v dopravě či územním plánu,\" dodal Novotný. Problémy mohou ještě nastat u obsazování postů v představenstvech a dozorčích radách městských firem. V případě metropolitního plánu je klíčový nesoulad mezi Matějem Stropnickým (SZ) a exprimátorem Hudečkem, kteří musí hledat kompromis právě nad tímto dokumentem. Jeden považuje Metropolitní plán za dokončený a připravený k projednání, druhý ho chce přepracovat. V dopravě se liší pohledy na to, zda má město tlačit na dokončení městského okruhu v navrhované variantě a jakou podobu by měly mít nové části vnějšího okruhu. TOP 09 by je stavěla podle aktuálních plánů, Trojkoalice naopak prosazuje v některých případech změnu tras.Problémy se systémem obsazování představenstev a dozorčích rad městských či poloměstských firem měli hlavně piráti, kteří z nich původně chtěli \"vymýtit\" politiky.\"Nám jde hlavně o nastavení jasných pravidel obsazování těchto orgánů,\" řekl k tomu v pondělí jejich lídr Jakub Michálek. Do představenstev by usedali manažeři (politici jen výjimečně) a do dozorčích rad i politici (pokud možno s nějakou odbornou znalostí věci).Napínavý seriál o skládání koalice na magistrátu by se v následujících týdnech mohl odvíjet v zásadě dvojím způsobem. Zaprvé: Když TOP 09 \"a spol.\" uspěje, může mít Praha nového primátora v osobách exnáměstků buď Nouzy, nebo Novotného.Zadruhé: Není vyloučené, že spolupráce TOP 09, Trojkoalice, ODS a pirátů ztroskotá na programu, když se nedohodnou například na projektu Metropolitního plánu či na dopravě. Navíc sílí hlasy zastupitelů, kterým se do spojenectví moc nechce. Například ODS vadí angažmá Tomáše Hudečka a část klubu TOP 09 je rozpačitá z pirátů.\"Této koalici proto někteří zastupitelé říkají mediální - protože je líbivější navenek. Je možné, že to nakonec dopadne úplně jinak,\" řekl MF DNES jeden ze zdrojů. Přál si zůstat v anonymitě stejně jako jiní zastupitelé. Část z nich totiž preferuje koalici nazývanou \"praktická\", v níž by měla TOP 09 spolupracovat s hnutím ANO, ČSSD a případně ještě s dalším partnerem.Nebo se tandem ANO a ČSSD obrátí na Trojkoalici a urovnají spory. To je ostatně varianta, o níž někteří radní hovořili už pár minut po listopadovém rozpadu vlády.Původní rada ANO, ČSSD a Trojkoalice se rozpadla potom, co se strany rozhádaly a posléze si vzájemně odvolaly několik radních.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\"Koaliční jednání na magistrátu: TOP 09, ODS, Trojkoalice a piráti hledají společnou cestu\"<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = new_model.generate(\n",
    "        input_ids=example_encoded,\n",
    "        max_new_tokens=256,\n",
    "#         num_beams=5,\n",
    "        do_sample=True,\n",
    "#         top_k=10,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "#         top_p=0.975,\n",
    "#         temperature=0.1,\n",
    "#         repetition_penalty=1.2,\n",
    "#         penalty_alpha=0.6,\n",
    "        num_return_sequences=1,\n",
    "#       eos_token_id=terminators,\n",
    "        eos_token_id=terminators,\n",
    "    )\n",
    "op = new_tokenizer.decode(generation_output[0], skip_special_tokens=False)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prvé trénovanie:\n",
    " -   alpha: 16, r: 8, batch_size: 4\n",
    " - výsledky: nanič\n",
    " - pri trénovaní sa loss postupne znižovala z 3.1 na 1.7, nasledoval spike na cca 2.8 potom loss postupne klesal \n",
    " - a toto sa opakovalo do konca (250 krokov, cca 4 takéto spiky)\n",
    " \n",
    "Ďalšie pokusy:\n",
    " - len ABSTRACT2HEADLINE\n",
    " - alpha: 16, r: 64, batch_size:8, group_by_length = False\n",
    " - opäť neúspech, menej oscilácie v loss\n",
    " \n",
    " - možno zaujímavé pozireť aj: https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/peft-flan-t5-int8-summarization.ipynb\n",
    "   - trénujú to cez Sequence2SequenceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='csmpt7b-ft-SumeCzech', vocab_size=64000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '[EOS]', 'unk_token': '[UNK]', 'pad_token': '[EOS]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64000: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64001: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.decode(torch.tensor([2]), skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "046b7d5b13104068b60e0b67f7c8313d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16a9c451d8424877ba84bd021334e9ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "234bc12a982c41ae93b93156856c3a81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ea80c14f1314b5c95fa997b67420dc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45c95b08e74845af90fe05f1ae826e14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bd047b485aa4e6d86cd4cda101cc98f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5ba881790f914e77ab171d906abc9d8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d993c8164da4d42b1d1a00a171dd483": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45c95b08e74845af90fe05f1ae826e14",
      "placeholder": "​",
      "style": "IPY_MODEL_234bc12a982c41ae93b93156856c3a81",
      "value": " 10000/10000 [00:20&lt;00:00, 560.20 examples/s]"
     }
    },
    "67f7b7125a4e42aabdd3614b3f8a079d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6be2100c4c7f4605a4ed20d59627a830": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_754e9f27204242f09fa7a3f59373512e",
       "IPY_MODEL_71bf17a0908d48b4ad8e444ea014398f",
       "IPY_MODEL_5d993c8164da4d42b1d1a00a171dd483"
      ],
      "layout": "IPY_MODEL_fc0a449394a4422abb814f4f1e8d9319"
     }
    },
    "71bf17a0908d48b4ad8e444ea014398f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cfc37e675eb420590003bdd30eef482",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16a9c451d8424877ba84bd021334e9ee",
      "value": 10000
     }
    },
    "7486ae8153d444a6b57df2ca840490ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f8abfefc24942f4ad5fd62262b06168",
      "placeholder": "​",
      "style": "IPY_MODEL_2ea80c14f1314b5c95fa997b67420dc9",
      "value": " 3/3 [01:08&lt;00:00, 22.76s/it]"
     }
    },
    "754e9f27204242f09fa7a3f59373512e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9745305d1cf6400e985450fcee4cca12",
      "placeholder": "​",
      "style": "IPY_MODEL_d3bb4824364e485aa81859e4762df7a2",
      "value": "Map: 100%"
     }
    },
    "7cfc37e675eb420590003bdd30eef482": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f26b29ee13c4f6e95e8cd723410718f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67f7b7125a4e42aabdd3614b3f8a079d",
      "placeholder": "​",
      "style": "IPY_MODEL_046b7d5b13104068b60e0b67f7c8313d",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "8f8abfefc24942f4ad5fd62262b06168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9745305d1cf6400e985450fcee4cca12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb2a038fcda44e4da4d89e3767fa8911": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0f505d1cc81485989225fb328bc396a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ba881790f914e77ab171d906abc9d8d",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4bd047b485aa4e6d86cd4cda101cc98f",
      "value": 3
     }
    },
    "d3bb4824364e485aa81859e4762df7a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3dd5ef4623e406daa5d2f019570f3b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f26b29ee13c4f6e95e8cd723410718f",
       "IPY_MODEL_d0f505d1cc81485989225fb328bc396a",
       "IPY_MODEL_7486ae8153d444a6b57df2ca840490ee"
      ],
      "layout": "IPY_MODEL_cb2a038fcda44e4da4d89e3767fa8911"
     }
    },
    "fc0a449394a4422abb814f4f1e8d9319": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
